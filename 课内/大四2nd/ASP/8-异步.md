# Content

第8讲讨论了coroutines和异步编程。我们回顾了多线程应用程序在阻塞I/O方面的局限性，并讨论了使用协程（coroutines）将几个并发的非阻塞I/O操作复用到一个线程上。讨论了由此产生的异步编程模型，以及它的好处和限制。

* [Content](#content)
* [1Motivation](#1motivation)
* [阻塞IO：Blocking I/O](#阻塞ioblocking-io)
* [解决阻塞IO-多线程:Blocking I/O using Multiple Threads](#解决阻塞io-多线程blocking-io-using-multiple-threads)
* [非阻塞IO-轮询:Non-Blocking I/O and Polling](#非阻塞io-轮询non-blocking-io-and-polling)
* [C-select()](#c-select)
* [非阻塞IO替代：Alternatives to Non-blocking I/O](#非阻塞io替代alternatives-to-non-blocking-io)
* [2async & await](#2async--await)
* [协程&异步代码：Coroutines and Asynchronous code](#协程异步代码coroutines-and-asynchronous-code)
* [编程模型：Programming Model](#编程模型programming-model)
* [async Functions](#async-functions)
* [await Future Results](#await-future-results)
* [async & await编程模型：Programming models](#async--await编程模型programming-models)
* [运行时支持：Runtime Support](#运行时支持runtime-support)
* [3异步设计模式：Design Patterns for Asynchronous Code](#3异步设计模式design-patterns-for-asynchronous-code)
* [====================](#)
* [组合Future:Compose Future values](#组合futurecompose-future-values)
* [避免阻塞操作：Avoid Blocking Operations](#避免阻塞操作avoid-blocking-operations)
* [避免长期运行计算：Avoid Long-running Computations](#避免长期运行计算avoid-long-running-computations)
* [何时使用异步IO：When to use](#何时使用异步iowhen-to-use)
* [异步IO效率：Performance](#异步io效率performance)
* [====================](#-1)

# 1Motivation

讲座的第一部分讨论了使用协程和异步编程的动机。它回顾了由阻塞I/O操作引起的问题，这些问题如何导致多线程代码的使用，以及如何影响程序的结构。

# 阻塞IO：Blocking I/O

```rust
fn read_exact<T: Read>(input: &mut T, buf: &mut [u8]) -> Result<(), std::io::Error> { 
    let mut cursor = 0;
    while cursor < buf.len() {
        cursor += input.read(&mut buf[cursor..])?;
    }
}
```

- **希望能与其他操作同时进行I/O操作**
  - I/O操作很慢
    - 需要等待网络、磁盘等方面的操作 - 操作可能需要数百万个周期
  - I/O操作会阻塞线程
    - 扰乱用户体验，阻碍其他计算的进行
- **想让I/O和计算重叠在一起**
- **希望允许多个并发的I/O操作**

> read_exact()函数。这个函数正在做的是，从某个输入流中读取精确的数据量，并将其存储在一个缓冲区中。例如，它可能是从一个文件中读取，也可能是从一个套接字中读取。the read_exact() function. What this function is doing, is reading an exact amount of data from some input stream, and storing it in a buffer. For example, it might be reading from a file, or it might be reading from a socket.
> 例如，如果它从一个TCP套接字中读取，那么根据拥塞控制，根据发送者正在做什么，套接字有可能没有所要求的数据量可供读取。因此，这个函数必须在一个循环中工作，重复地从套接字中读取数据，直到它收到所要求的数据量。这很有效，而且很简单，很直接。不过，这种类型的代码的问题是双重的。 If it's reading from a TCP socket, for example, then it it's possible, depending on the congestion control, depending on what the sender is doing, it’s possible that the socket doesn't have the requested amount of data available to read. And so this function has to work in a loop, repeatedly reading from the socket until it's received the amount of data that that's been requested. And this works, and it's simple and it's straightforward. The problem with this type of code, though, is twofold.
> 首先，I/O操作可能非常慢。当从文件描述符中读取时，该函数可能会阻塞很长时间。这可能只是因为磁盘很慢，也可能是因为网络很慢，还可能是因为它被阻塞了，因为它正在从网络上读取，而没有任何可用的数据，它必须等待发送者发送更多的数据。所以调用read()函数()可能需要数百万个周期。它们可能是令人难以置信的慢。 Firstly, the I/O operations can be very slow. The function can block for a long time, while reading from the file descriptor. And that could just be because the disk is slow, or it could be because the network is slow, or it could be because it's blocked because it's reading from the network and there's nothing available to read and it has to wait for the sender to send more data. So calls to the read() function() can take many millions of cycles. They can be incredibly slow
> 这里的另一个问题是，当read()函数被调用时，它会阻塞线程。执行线程在等待那段时间时停止，而read()的调用正在完成。而这阻碍了其他计算的运行，如果该线程也是你的用户界面的一部分，它就会扰乱用户体验。. The other issue here is that when the read() function is called, it blocks the thread. The thread of execution stops while waiting for that period of time, while the read() call is completing. And this prevents other computations from running, and if the thread is also part of your user interface, it disrupts the user experience.
> 因此，理想情况下，我们希望能够将I/O和计算重叠起来。我们希望能够同时运行它们。理想情况下，希望能够允许多个并发的I/O操作。 So, ideally, we want to be able to overlap the I/O and the computation. We want to be able to run them concurrently. Ideally, want to be able to allow multiple concurrent I/O operations.

# 解决阻塞IO-多线程:Blocking I/O using Multiple Threads

```rust
fn main() {
    …
    let (tx, rx) = channel(); 
    thread::spawn(move|| {
        …perform I/O…
        tx.send(results);
    });
    …
    let data = rx.recv();
    …
}
```

- 传统的解决方法是将阻塞性操作转移到独立的线程中。•	Traditionally solved by moving blocking operations into separate threads:
- 生成专门的线程来并发地执行I/O操作•	Spawn dedicated threads to perform I/O operations concurrently
- 完成后重新加入主线程/将结果作为消息传回•	Re-join main thread/pass back result as message once complete

> 当然，传统上解决这个问题的方法是使用多线程。通常的解决方案是将阻塞的I/O操作从主线程中移出，放到一个单独的线程中，由该线程进行I/O操作，然后在数据可用时再报告。The way this has traditionally been solved, of course, is by using multiple threads. The usual solution to this, is to move the blocking I/O operations out of the main thread, and put them into a separate thread which does the I/O, and then reports back once the data is available. 
> 在Rust中，你可以通过幻灯片上的代码来完成这个任务。你创建一个通道，然后生成一个线程来执行I/O。然后这个线程将结果送回通道，程序的其他部分继续进行，并与I/O操作的执行相重叠。And, in Rust, the way you might do this is outlined in the code on the slide. You create a channel, and you spawn a thread to perform I/O. And that thread then sends the results back down the channel, and the rest of the program continues, and overlaps with the execution of the I/O operation

---

优点。• Advantages:

- 简单•	Simple
  - 没有新的语言或运行时特性•	No new language or runtime features
  - 不需要改变我们做I/O的方式•	Don’t have to change the way we do I/O
  - 必须将I/O转移到一个单独的线程，进行通信和同步•	Do have to move I/O to a separate thread, communicate and synchronise
- 如果系统有多个内核，并发代码可以并行运行•	Concurrent code can run in parallel if the system has multiple cores
- 安全，如果使用Rust，由于所有权规则可以防止数据竞争•	Safe, if using Rust, due to ownership rules preventing data races

> 这相对简单，因为它实际上并不需要任何新的语言或运行时特性。这是相同的阻塞调用。它是相同的多线程函数，无论如何我们都有。它并没有真正改变我们进行 I/O 的方式。它不会改变 I/O 或线程的基本模型。但这确实意味着我们必须将 I/O 代码移至单独的线程，这是一些编程开销。And this is relatively simple, in that it doesn't really require any new language or runtime features. It’s the same blocking calls. it's the same multithreading functions, we have anyway. And it doesn’t really change the way we do I/O. It's not changing the fundamental model of either I/O or threading. But it does mean we have to move the I/O code to a separate thread, which is some programming overhead.
> 不过，它也有一个好处，那就是如果系统真的有多个核心，它可以并行运行。而且它很安全，特别是在Rust中，因为所有权规则可以防止数据竞赛，所以很容易，而且相对直接，就可以把代码推到一个单独的线程。 It also has the advantage, though, that it can run in parallel if the system really does have multiple cores. And it's safe, especially in Rust, because the ownership rules prevent data races so it's easy, and relatively straightforward, to push the code into a separate thread.

---

缺点。•	Disadvantages:

- **复杂**•	Complex
  - 需要将应用程序分割成多个线程•	Requires partitioning the application into multiple threads
- **资源紧张**•	Resource heavy
  - 每个线程都有自己的堆栈•	Each thread has its own stack
  - 上下文切换的开销•	Context switch overheads
- **并行性对I/O的好处有限**•	Parallelism offers limited benefits for I/O
  - 执行I/O的线程通常会在大部分时间内被封锁•	Threads performing I/O often spend majority of time blocked
  - 启动一个新的线程，而这个线程的大部分时间都在做无用功，这是一种浪费。•	Wasteful to start a new thread that spends most of its time doing nothing

> 缺点是它增加了复杂性。创建一个线程和Rust并不难。但它比不创建线程更难。生成一个线程，将I/O操作划分到一个单独的线程中，在Rust中概念上并不困难，但它使代码变得复杂。这是一个更复杂的编程模型。它使代码的结构变得模糊不清。它的资源相对较重。你有上下文切换到独立线程的开销，每个线程都有自己的栈，自己的内存开销。这不是一个巨大的开销，但它是一个开销。这比在主线程中运行read()函数的开销要大得多。The disadvantages are that it adds complexity. Creating a thread and Rust isn't difficult. But it's harder than not creating a thread. Spawning a thread, partitioning the I/O operations into a separate thread, isn't conceptually difficult in Rust, but it complicates the code. It's a more complex programming model. It obfuscates the structure of the code. It's relatively resource heavy. You have the overheads of context switching to the separate threads, and each thread has its own stack, its own memory overhead. And that's not an enormous overhead, but it is an overhead. And it's a much heavier overhead than just running the read() call within the main thread.
> 事实上，线程可以并发运行，如果你有多核硬件，线程可以并行运行，这实际上是一个相对有限的好处，因为无论如何，线程的大部分时间都是阻塞在等待I/O。因此，启动一个新的线程，只是为了调用一个读取()函数，然后阻塞其90%的时间，这有点浪费。 And the fact that the threads can run concurrently, the fact that the threads can run in parallel if you have multicore hardware, is actually a relatively limited benefit, because the thread spends most of its time blocked waiting for I/O anyway. So it's a bit of a waste starting a new thread, just to call a read() function which then blocks 90% of its lifetime.

# 非阻塞IO-轮询:Non-Blocking I/O and Polling

使用线程进行阻塞式I/O是有问题的。•	Blocking I/O using threads is problematic:

- **线程提供了并发的I/O抽象，但开销很大**•	Threads provide concurrent I/O abstraction, but with high overhead
  - 多线程可以是低成本的→Erlang •	Multithreading can be inexpensive → Erlang
  - 但在通用的操作系统上有很高的开销•	But has high overhead on general purpose operating systems
    - 由于安全要求，上下文切换的开销较大•	Higher context switch overhead due to security requirements
    - 由于单独的堆栈，内存开销较高•	Higher memory overhead due to separate stack
    - 由于更多的隔离、抢占式调度，开销更大•	Higher overhead due to greater isolation, preemptive scheduling
- **与I/O绑定的代码并行的机会有限**•	Limited opportunities for parallelism with I/O bound code
  - 线程可以并行调度，但除非与CPU绑定，否则益处不大•	Threads can be scheduled in parallel, but to little benefit unless CPU bound

> 所以我们当然可以使用多线程来执行阻塞式I/O。但这是有问题的。在很多情况下，它的开销很大。你有上下文切换的开销，你有独立堆栈带来的内存开销，你有调度器的开销，反正**并行的好处不多，因为它大部分时间都是阻塞。而有些系统允许你避免这种情况。例如，在Erlang中，线程是相当轻量级的，但在大多数系统中，情况并非如此，启动多个线程来执行I/O是一个相对较高的开销**。 So we can certainly perform blocking I/O using multiple threads. But it's problematic. It’s high overhead in a lot of cases. You've got context switch overheads, you've got the memory overheads due to the separate stack, you've got the scheduler overheads, and there’s not that much benefit of parallelism anyway, because it blocks for most of the time. And some systems allow you to avoid this. In Erlang, for example, the threading is pretty lightweight, but in most systems this is not the case, and it's a relatively high overhead to start multiple threads to perform I/O.

---

轻量级的替代方案：在单个线程中复用I/O操作•	Lightweight alternative: multiplex I/O operations within a single thread

- I/O操作是异步完成的--为什么要让线程为其阻塞？•	I/O operations complete asynchronously – why have threads block for them?
- 提供一种机制来启动异步I/O，并为I/O事件轮询内核--所有这些都在一个应用线程内进行•	Provide a mechanism to start asynchronous I/O and poll the kernel for I/O events – all within a single application thread
- 启动一个I/O操作•	Start an I/O operation
- 定期轮询I/O操作的进展•	Periodically poll for progress of the I/O operation
- 如果有新的数据，发送操作已经完成，或者发生了错误，那么就调用该操作的处理程序•	If new data is available, a send operation has completed, or an error has occurred, then invoke the handler for that operation

> 我们希望有更轻的东西作为替代。我们希望以某种方式能够在一个线程中复用I/O操作，我们希望以某种方式允许I/O操作异步完成，而不需要有单独的控制线程。We'd like we'd like something more lightweight as an alternative. We'd like to somehow be able to multiplex I/O operations in a single thread, we'd like to somehow allow I/O operations to complete asynchronously, without having to have separate threads of control.
> 因此，我们应该提供一种机制，允许我们启动异步I/O，启动一个I/O操作，让它在后台运行，并允许我们以某种方式轮询内核，看它是否已经完成，所有这些都在一个应用线程中运行。 So, it would be desirable to provide a mechanism which allows us to start asynchronous I/O, start an I/O operation, and let it run in the background, and somehow allow us to poll the kernel to see if it has finished yet, all running within a single application thread. 
> 我们的想法是，你启动I/O，然后在程序继续执行其他计算时，它在后台继续运行。然后，在某些时候，一旦数据可用，要么有一个回调被执行以提供数据，要么主线程可以直接轮询并说 "它完成了吗？"，如果它完成了，它可以把数据拉进来。The idea would be that you start the I/O, and then it continues in the background while the program continues performing other computations. And then, at some point, once the data is available, either there's a callback which gets executed to provide the data, or the main thread can just poll it and say “has it finished?”, and if it has, it can pull the data in.

---

已存在的轮询I/O机制•	Mechanisms for polling I/O for readiness

- C语言中的Berkeley Sockets API select()函数 •	Berkeley Sockets API select() function in C
- 或更高性能，但便携性较差的变体，如epoll（Linux/Android）、kqueue（FreeBSD/ macOS/iOS）、I/O完成端口（Windows）。•	Or higher-performance, but less portable, variants such as epoll (Linux/Android), kqueue (FreeBSD/ macOS/iOS), I/O completion ports (Windows)
- 库，如libevent、libev或libuv--此类系统服务的通用API •	Libraries such as libevent, libev, or libuv – common API for such system services
- Rust mio库

> 而这也是一个相当常见的抽象概念。如果你是一个C语言程序员，这就是伯克利套接字API中的select()函数，比如说。还有一些新的、更高性能的版本，比如epoll()，如果你是一个Linux或Android程序员，或者FreeBSD中的kqueue抽象，或者macOS，或者iPhone上的。或者，如果你是一个Windows程序员，I/O完成端口做了非常类似的事情。而这往往会被包裹在库中，比如libevent，或者libev，或者libuv，它试图为它们提供通用的可移植API。而且，在Rust中，mio库也为其提供了一个可移植的抽象。And this is also a reasonably common abstraction. If you’re a C programmer, this is the select() function in the Berkeley sockets API, for example. And there's a bunch of new, higher performance, versions of this, such as epoll(), if you're a Linux or an Android programmer, or the kqueue abstraction in FreeBSD, or macOS, or on the iPhone. Or, if you're a Windows programmer, I/O completion ports do something very similar. And this tends to get wrapped in libraries, such as libevent, or libev, or libuv, which was trying to provide common portable APIs for them. And, in Rust, the mio library also provides a portable abstraction for this.

关键功能。•	Key functionality:

- 触发非阻塞的I/O操作：对文件、套接字等进行读（）或写（）。•	Trigger non-blocking I/O operations: read() or write() to files, sockets, etc.
- 轮询内核，检查是否有可读或可写的数据，或者是否有未解决的错误•	Poll kernel to check for readable or writeable data, or if errors are outstanding
- 高效，只需要一个单线程，但需要重组代码以避免阻塞•	Efficient and only requires a single thread, but requires code restructuring to avoid blocking

>  这些东西所提供的功能，是触发非阻塞操作的能力。它们为你提供了一种从文件或套接字中说 "异步读取 "或 "异步写入 "的方法，并提供了一个poll()抽象，所以你可以定期检查它是否完成并检索数据。它们实际上是相当有效的。它们**满足了效率的目标，满足了只在单线程中运行的目标，而且它们建立在提供异步I/O的操作系统内核的功能上。它们的问题是，它们需要再次重组代码以避免阻塞**。 The functionality that these things provide, is the ability to trigger non-blocking operations. They provide you a way of saying “read asynchronously” or “write asynchronously’ from a file or a socket, and they provide a poll() abstraction, so you can periodically check to see if it completed and retrieve the data. And they're actually pretty efficient. They meet the goals of efficiency, they meet the goals of only running in a single thread, and they build on features of the operating system kernels that provide asynchronous I/O. The problem with them, is that they require, again, restructuring the code to avoid blocking.

# C-select()

•	Berkeley Sockets API select() function in C:

```c
FD_ZERO(&rfds);
FD_SET(fd1, &rfds); 
FD_SET(fd2, &rfds);

tv.tv_sec = 5;// Timeout
tv.tv_usec = 0;

int rc = select(1, &rfds, &wfds, &efds, &tv);
if (rc < 0) {
    … handle error
} else if (rc == 0) {
    … handle timeout
} else {
    if (FD_ISSET(fd1, &rfds)) {
        … data available to read() on fd1
    }
    if (FD_ISSET(fd2, &rfds)) {
        … data available to read() on fd2
    }
    …
}
```

* select()对一组文件描述符进行轮询，看它们是否准备好进行读取()、写入()，或传递错误。select() polls a set of file descriptors for their readiness to read(), write(), or to deliver errors
* FD_ISSET()在select()之后检查特定的文件描述符是否准备就绪。FD_ISSET() checks particular file descriptor for readiness after select()
* 低级别的API很适合C语言编程；其他库/语言提供类似的功能 Low-level API well-suited to C programming; other libraries/languages provide comparable features

> 这是一个例子。这是使用套接字和C语言的select()函数的网络代码。我们在这里看到的是select()的调用，你把三个参数传给它，一组可读的文件描述符，一组可写的文件描述符，一组可能出错的文件描述符，以及一个超时。你必须把所有可能有未完成的异步I/O的文件描述符捆绑起来，把它们填入这些参数，调用它，然后依次轮询每个文件描述符，看看FD_ISSET调用，看看这些不同的文件描述符中，有哪些数据可以读或写。And this is an example. This is network code using sockets and select() function in C. And what we see here is the select() call, where you pass it the three parameters, the set of readable file descriptors, the set of writeable file descriptors, the set of file descriptors might deliver errors, and a timeout. You have to bundle up all the file descriptors that may have outstanding asynchronous I/O, fill them into these parameters, call it, and then poll each of these in turn to see if the FD_ISSET calls, to see which of those different file descriptors, have data available to read or write.
> 而这是一个相对低级的API，合理地适合于C语言编程，但它确实需要对代码进行相当的重组。这不再是简单的作为一个循环的一部分调用read()。它是将整个程序重组为一个事件循环，你在不同的文件描述符和不同的套接字上轮询。 And it's a relatively low-level API, which is reasonably well suited to C programming, but it does require quite a restructure of the code. This is no longer as simple as just calling read() as part of a loop. It’s restructuring the whole program as an event loop, where you poll on different file descriptors, different sockets.
> 我提到的替代库，比如用于C语言编程的libuv，比如用于Rust编程的mio，使其更具有可移植性，而且它们的水平更高一些，它们去掉了一些模板，但从概念上来说，你在做同样的事情。从概念上讲，你必须将程序重组为一个事件循环，在那里你触发异步I/O，每隔一段时间你就轮询一下，看看它是否完成。这涉及到代码的重组。 And the alternative libraries I mentioned, such as libuv for C programming, such as mio for Rust programming, make this more portable, and they're a little bit higher level, and they remove some of the boilerplate, but conceptually you're doing the same thing. Conceptually you have to restructure the program as an event loop, where you trigger the asynchronous I/O, and every so often you poll it to see if it's completed. And it involves restructuring the code.

# 非阻塞IO替代：Alternatives to Non-blocking I/O

- **非阻塞式I/O可以是高效的**•	Non-blocking I/O can be highly efficient
  - 单个线程可以同时处理多个I/O源（套接字、文件描述符）。•	Single thread can handle multiple I/O sources (sockets, file descriptors) at once
- **但是--需要大量重写应用程序代码**•	But – requires significant re-write of application code
  - 非阻塞式I/O•	Non-blocking I/O
  - 对I/O源进行轮询•	Polling of I/O sources
  - 数据的重新组合•	Re-assembly of data

- 我们能否以一种更可用的方式获得非阻塞I/O的效率？•	Can we get the efficiency of non-blocking I/O in a more usable manner?
  - 也许--**协程和异步代码**•	Maybe – coroutines and asynchronous code

> 现在，这些方法有一个优势，就是它们非常高效。因为异步是由操作系统内核处理的，一个线程可以非常有效地处理多个套接字。它所做的只是触发异步操作，而内核线程则处理所有其他的事情。内核中内置了并发运行这些操作的机制，它们是相当高效的。Now, these approaches have the advantage that they're very efficient. Because the asynchrony is handled by the operating system kernel, a single thread can very efficiently handle multiple sockets. All it does is trigger the asynchronous operation, and a kernel thread handles all the rest. The mechanisms to run these operations concurrently are built into the kernel, and they're pretty efficient. 
> 但这需要我们重写应用程序代码。它要求我们将应用程序重组为一个看起来不同的东西。作为一个有事件循环的东西，它轮询数据源并重新组合数据。But it requires us to rewrite the application code. It requires us to restructure the application as something which looks different. As something which has an event loop, which polls the data sources and reassembles the data. 
> 因此，从根本上说，我们有两个选择。我们可以将代码结构化为一组多线程，这涉及到生成一个线程，并将代码重组为一组多线程，一旦我们成功读取数据，就将数据传回。So, fundamentally, we have two choices. We can structure the code as a set of multiple threads, which involves spawning a thread and restructuring the code as a set of multiple threads which pass the data back once we've successfully read the data. 
> 或者我们可以使用异步I/O原语来重组代码，这涉及到将其变成一个带有轮询的事件循环，等等。Or we can restructure the code using the asynchronous I/O primitives, which involves turning it into an event loop with polling and so on. 
> 同样，这两种方式都涉及对代码进行相当基本的重写，以获得这些效率增益。我们想要的是能够以更实用的方式获得这种效率，获得非阻塞 I/O 的效率。这个想法是协程和异步代码是这样做的一种方式。Again, both ways involve a fairly fundamental rewrite of the code to get the these efficiency gains. What we would like is to be able to get this efficiency, get the efficiency of non-blocking I/O, in a much more usable manner. And the idea is that coroutines and asynchronous code are one way of doing that, 

# 2async & await

讲座的第二部分讨论了如何利用coroutines和异步代码来支持单线程上的I/O复用。它回顾了什么是coroutine，以及coroutine可以通过重复产生结果而对其调用者并发执行的方式；它还回顾了这是如何实现的。概述了这种方式可以用来支持异步I/O操作，从而对Python和Rust中的异步函数进行了描述。对运行时支持的需求被概述。

# 协程&异步代码：Coroutines and Asynchronous code

旨在以更自然的方式为单线程上的I/O复用提供语言和运行时支持•	Aims to provide language and run-time support for I/O multiplexing on a single thread, in a more natural style

```rust
fn read_exact<T: Read>(input: &mut T, buf: &mut [u8]) -> Result<(), std::io::Error> {
     let mut cursor = 0;
    while cursor < buf.len() {
        cursor += input.read(&mut buf[cursor..])?;
    }
}

↓

async fn read_exact<T: AsyncRead>(input: &mut T, buf: &mut [u8]) -> Result<(), std::io::Error> { 
    let mut cursor = 0;
    while cursor < buf.len() {
        cursor += input.read(&mut buf[cursor..]).await?;
    }
}
```

运行时在线程池上安排异步函数，在等待调用时让位于其他代码。等待调用→低开销的并发I/O •	Runtime schedules async functions on a thread pool, yielding to other code on await calls → low-overhead concurrent I/O

> 像我们在幻灯片顶部的例子中看到的那样，调用阻塞式I/O函数的代码，如read()，在等待这些调用完成时，会使程序的执行停滞。而解决这个问题的方法是使用多线程或异步I/O函数，如select()，需要对代码进行广泛的重组。Code, like we see in the example at the top of the slide, that calls blocking I/O functions, such as read(), stalls the execution of the program while waiting for those calls to complete. And the work-arounds for this, using multiple threads or asynchronous I/O functions, such as select(), require extensive restructuring of the code. 
> 在异步代码中使用协程的目的是让I/O和计算在一个线程上同时进行，而不需要重组代码。希望这能避免多线程的开销，同时保留原有的代码结构。The goal of using coroutines with asynchronous code is to allow I/O and computation to be performed concurrently on a single thread, without restructuring the code. The hope is that this will avoid the overheads of multithreading, while retaining the original code structure. 
> 从本质上讲，我们希望将幻灯片顶部所示的阻塞代码转化为异步的、非阻塞的代码，如底部所示。并为语言的运行时间提供能力，使其能够与低开销的异步I/O操作同时执行这些异步函数。Essentially we hope to transform the blocking code shown at the top of the slide, into the asynchronous, non-blocking, code such as that shown at the bottom. And provide the language runtime with the ability to execute those asynchronous functions concurrently with low-overhead asynchronous I/O operations.

# 编程模型：Programming Model

将基于I/O的代码结构化为一组接受来自I/O源的数据并代替阻塞而产生的并发协程。	Structure I/O-based code as a set of concurrent coroutines that accept data from I/O sources and yield in place of blocking

什么是coroutine？

* 一个生成器产生一连串的值。A generator yields a sequence of values:
* ![](/static/2022-05-08-15-26-35.png)
* **一个可以重复运行的函数，产生一连串的值，同时保持内部状态**。A function that can repeatedly run, yielding a sequence of values, while maintaining internal state
* 调用 countdown(5) 产生一个生成器对象。for循环协议在该对象上调用next()，使其执行到下一个产生语句并返回产生的值。Calling countdown(5) produces a generator object. The for loop protocol calls next() on that object, causing it to execute until the next yield statement and return the yielded value.
* 堆分配；保持状态；仅响应外部刺激而执行 Heap allocated; maintains state; executes only in response to external stimulus

> 我们所考虑的编程模型将代码结构化为一组并发协程，接受来自I/O源的数据，并代替阻塞的产生。COROUTINE并发执行，重叠I/O和计算，都在一个执行线程中，因此避免了多线程的开销。为了理解这种编程模型，我们必须首先问什么是coroutine？The programming model we’re considering structures the code as a set of concurrent coroutines that accept data from I/O sources and yield in place of blocking. The coroutines execute concurrently, overlapping I/O and computation, all within a single thread of execution, and so avoid the overhead of multithreading. To understand this programming model, though, we must first ask what is a coroutine? 
> 如果我们考虑一个普通的函数，我们会看到它被调用，执行一段时间，然后返回一个结果。相比之下，coroutine有能力暂停其执行。它不是返回一个单一的值，甚至是一个值的列表，而是懒洋洋地生成一个值的序列。if we consider a normal function, we see that it’s called, executes for a while, and returns a result. A coroutine, in contrast, has the ability to pause its execution. Rather than return a single value, or even a list of values, it lazily generates a sequence of values. 
> 幻灯片显示了一个用Python编写的例子。在这个例子中，countdown()函数是一个coroutine，它产生了一个整数序列，从给定的参数值倒数到零。我们看到，用参数5调用countdown()，可以得到5、4、3、2和1的值，这些值可以由for循环处理。重要的是，countdown()函数不是返回一个单一的值，而是由一个倒数的数字列表组成。The slide shows an example, written in Python. In this case, the countdown() function is a coroutine that yields a sequence of integers, counting down to zero from the value given as its argument. We see that calling countdown() with the parameter 5, yields the values 5, 4, 3, 2, and 1, and that these can be processed by a for loop. Importantly, the countdown() function is not returning a single value, comprising a list of numbers counting down. 
> 相反，调用 countdown() 会返回一个生成器对象。就其本身而言，生成器对象什么都不做。但是生成器对象实现了一个 next() 方法。而 Python 中的 for 循环协议需要一个生成器对象并重复调用 next() 方法。每次调用 next() 的时候，函数都会执行，直到到达 yield 语句，然后返回下一个值。或者它一直执行到函数结束，这时它返回 None 以表示生成器已经完成。从本质上讲，这个函数变成了一个堆分配的生成器对象，它保持着状态，在响应对next()的调用时懒洋洋地执行，并重复地产生不同的值。Rather, calling countdown() returns a generator object. By itself, a generator object does nothing. But the generator object implements a next() method. And the for loop protocol in Python takes a generator object and repeatedly calls that next() method. And each time next() is called, the function executes until it reaches the yield statement, then returns the next value. Or it executes until the function ends, when it returns None to indicate that the generator has completed. Essentially, the function is turned into a heap allocated generator object that maintains state, executes lazily in response to calls to next(), and repeatedly yields the different values.

---

```python
def grep(pattern): 
    print(F“Looking for {pattern}”)
    while True:
        line = (yield)
        if pattern in line: 
            print line

>>> g = grep("python")
>>> g.next() Looking for python
>>> g.send("Yeah, but no, but yeah, but no")
>>> g.send("A series of tubes")
>>> g.send("python generators rock!") python generators rock!
>>>
```

* Coroutine更普遍地消耗和产生数值。A coroutine more generally consumes and yields values:
* COROUTINE的执行是为了响应 next()或send()的调用。The coroutines executes in response to next() or send() calls
* 对next()的调用使其执行，直到它下一次调用yield来返回一个值。Calls to next() make it execute until it next call yield to return a value
* 对send()的调用将一个值传给coroutine，由（yield）返回。Calls to send() pass a value into the coroutine, to be returned by (yield)

> 在 Python 中的 Coroutines 不做任何事情，直到代表 Coroutine 的生成器对象被调用 next() 函数。通常这是自动发生的，作为for循环操作的一部分，但是我们也可以手动调用它，正如我们在这里看到的。Coroutines in Python do nothing until the next() function is called on the generator object representing the coroutine. Normally this happens automatically, as part of the operation of a for loop, but we can also call it manually, as we see here. 
> 在这个例子中，grep()函数是一个循环程序，对g = grep("python")的调用实例化了生成器对象。In this example, the grep() function is a coroutine, and the call to g = grep(“python”) instantiates the generator object. 
> 但是实例化生成器并不会导致它的运行。例如，grep()函数开始时的print()调用直到我们调用next()方法时才会执行，这就迫使coroutine运行直到它产生。But instantiating the generator doesn’t cause it to run. The print() call at the start of the grep() function doesn’t execute until we call the next() method, for example, forcing the coroutine to run until it yields. 
> 在这种情况下，函数输出是为了消耗一个值，所以我们调用send()而不是next()，并传入一个值。我们反复这样做，每次传入不同的值，每次都会导致grep()函数中的while循环执行一次。In this case, the function yields to consume a value, so we call send() rather than next(), and pass in a value. And we do this repeatedly, passing in different values each time, and each time causing a single iteration of the while loop in the grep() function to execute.

---

**coroutine是一个与代码的其他部分并发执行的函数，但不是并行执行的**。A coroutine is a function that executes concurrently to – but not in parallel with – the rest of the code It is event driven, and can accept and return values

* 我们看到，coroutine是一个与其他代码同时执行的函数。它是事件驱动的。它只在运行时调用它的next()或send()方法时执行，这将导致它的执行继续进行，直到下一次产生，这时控制权就会转回运行时。We see that the coroutine is a function that executes concurrently to the rest of the code. It’s event driven. It only executes when the runtime calls its next() or send() method, which causes its execution to resume until it next yields, at which point control passes back to the runtime. 
* 在这些例子中，我们只是一次执行了一个单协程，但是完全可以启动几个不同的单协程，并让运行时循环，调用它们的next()方法。这将导致不同协程同时执行，每个协程执行一段时间，直到它进入下一个协程。In the examples, we’ve only had a single coroutine executing at once, but it’s entirely possible to start several different coroutines, and have the runtime loop, calling their next() methods. This will cause the different coroutines to execute concurrently, each one executing for a while until it yields to the next. 
* 这是一种有时被称为合作式多任务的方法。系统上下文在每次一个协程让出处理器时进行切换，如果它没有让出，它就继续运行。这就是微软Windows 3.1和Macintosh System 7处理多任务的方式。It’s an approach that’s sometime known as cooperative multitasking. The system context switches each time a coroutine yields the processor, and if it doesn’t yield, it keeps running. This is how Microsoft Windows 3.1, and the Macintosh System 7, handled multitasking.

---

- **一个异步函数就是一个coroutine**. •	An async function is a coroutine
  - 阻塞的I/O操作在代码中被标记为 "等待"，并在执行I/O时导致控制权传递给另一个coroutine. •	Blocking I/O operations are labelled in the code – await – and cause control to pass to another coroutine while the I/O is performed
- **提供无并行的并发性** •	Provides concurrency without parallelism
  - Coroutine并发操作，但通常是在一个单线程中。•	Coroutines operate concurrently, but typically within a single thread
  - await将控制权传递给另一个循环程序，并在等待的操作完成后安排一个唤醒。•	await passes control to another coroutine, and schedules a later wake-up for when the awaited operation completes
  - 通过调用select()或类似的方式，将其编码为一个状态机。•	Encodes down to a state machine with calls to select(), or similar
- **模仿多线程I/O的代码结构--在一个单线程内**•	Mimics structure of code with multi-threaded I/O – within a single thread

> 但它给我们提供了一个在单线程内有效处理I/O的基础。我们把在一个线程中执行的代码结构化为一组触发异步I/O操作的协程，并在I/O上产生而不是阻塞。 But it gives us a basis for efficient I/O handling within a single thread. We structure the code to execute within a thread as a set of coroutines that trigger an asynchronous I/O operation, and yield rather than blocking on I/O. 
> 我们将这些函数标记为async。这是一个标签，告诉语言运行时，这些函数是调用异步I/O操作的协程。通常会阻塞的I/O操作会在代码中标上一个await标签。这将导致coroutine触发该I/O操作的异步版本，然后产生，在执行I/O的同时将控制权传递给另一个coroutine。And we label the functions as being async. This is a label that tells the language runtime that those functions are coroutines that call asynchronous I/O operations. And I/O operations that would normally block are labelled in the code with an await tag. This causes the coroutine to trigger the asynchronous version of that I/O operation, then yield, passing control to another coroutine while the I/O is performed. 
> 这就提供了并发的I/O，而没有并行性。coroutine同时操作，但在一个单线程内。对await的调用告诉内核开始一个异步I/O操作，并将代表该操作的文件描述符交给运行时。运行时在一个循环中运行。它反复调用select()，并将产生任何可读或可写文件描述符的coroutine添加到运行队列的末端。然后，它在运行队列的首部继续运行协程，根据情况调用next()或send()方法。当该协程产生并返回一个文件描述符时，它就被移到被阻塞的任务列表中，其文件描述符被添加到将来被轮询的集合中。然后循环继续，再次调用select()。This provides concurrent I/O, without parallelism. The coroutines operate concurrently, but within a single thread. The calls to await tell the kernel to start an asynchronous I/O operation, and yield the file descriptor representing that operation to the runtime. The runtime operates in a loop. It repeatedly calls select(), and adds the coroutines that yielded any readable or writable file descriptors to the end of a run queue. Then, it resumes the coroutine at the head of the run queue, calling its next() or send() method as appropriate. And when that coroutine yields, and returns a file descriptor, it’s moved to the list of blocked tasks, and its file descriptor is added to the set to be polled in future. And the loop continues, calling select() again.

# async Functions

- 一个异步函数是一个可以作为一个循环协程的函数 •	An async function is one that can act as a coroutine
  - 它是由运行时异步执行的•	It is executed asynchronously by the runtime
  - 广泛支持 - Python 3, JavaScript, C#, Rust, ... •	Widely supported – Python 3, JavaScript, C#, Rust, …

```python
'''
在函数上的异步标签
yield → await
但本质上是一个coroutine
'''

#!/usr/bin/env python3 
import asyncio
async def fetch_html(url: str, session: ClientSession) -> str:
    resp = await session.request(method="GET", url=url)
    html = await resp.text()
    return html
```

- 主程序必须触发运行时的异步执行`asyncio.run(async function)`。•	Main program must trigger asynchronous execution by the runtime:
  - 启动异步轮询的运行时，运行到指定的异步函数完成为止•	Starts asynchronous polling runtime, runs until specified async function completes
  - 运行时驱动异步函数完成，并处理协程之间的切换•	Runtime drives async functions to completion and handles switching between coroutines

> 因此，一个异步函数是一个执行异步I/O操作的函数，它可以作为一个coroutine运行。幻灯片显示了Python中的一个例子。异步函数由运行时异步执行，以响应I/O事件，程序被写成一组异步函数。main()函数调用运行时，给它提供要运行的顶级异步函数。这就启动了异步运行时，开始轮询I/O，并运行到该异步函数完成。这是一个被广泛支持的模型。Coroutines，以异步函数的形式，存在于Python、JavaScript、C#、Rust和其他语言中。 An async function is therefore a function that performs asynchronous I/O operations and that can operate as a coroutine. The slide shows an example in Python. The async functions are executed asynchronously by the runtime, in response to I/O events, and the program is written as a set of async functions. The main() function calls into the runtime giving it the top-level async function to run. This starts the asynchronous runtime, starts it polling for I/O, and runs until that async function completes. It’s a widely supported model. Coroutines, in the form of async functions, exist in Python, JavaScript, C#, Rust, and in other languages.

# await Future Results

```python
'''
在函数上的异步标签
yield → await
但本质上是一个coroutine
'''

#!/usr/bin/env python3 
import asyncio
async def fetch_html(url: str, session: ClientSession) -> str:
    resp = await session.request(method="GET", url=url)
    html = await resp.text()
    return html
```

从协程中产生的一个await操作 An await operation yields from the coroutine

- **触发I/O操作 - 并将相应的文件描述符添加到运行时轮询的集合中•**	Triggers I/O operation – and adds corresponding file descriptor to set polled by the runtime
- **当文件描述符准备就绪时，将该程序放入队列，由运行时唤醒**。•	Puts the coroutine in queue to be woken by the runtime, when file descriptor becomes ready
- **如果另一个循环程序准备执行，那么一旦I/O完成，就安排唤醒，并将控制权传递给另一个循环程序；否则，运行时就会阻塞，直到这个或其他的I/O操作准备好**。•	If another coroutine is ready to execute then schedule wake-up once the I/O completes, and pass control passes to the other coroutine; else runtime blocks until either this, or some other, I/O operation becomes ready
- **在稍后的时间里，文件描述符准备好了，运行时就会重新安排该轮回程序--I/O完成后继续执行**•	At some later time the file descriptor becomes ready and the runtime reschedules the coroutine – the I/O completes and the execution continues

> **在一个异步函数中，await语句使函数在执行异步I/O操作的同时将控制权交给运行时。执行await语句会将控制权交给运行时。它将该程序放到一个队列中，以便在稍后的某个时间，当I/O操作完成时被唤醒。如果另一个cououtine准备执行，那么运行时就会安排产生函数在I/O完成后被唤醒，控制权就会转移到其他cououtine。否则，运行时就会阻塞，直到这个或其他的I/O操作准备就绪，然后将控制权传回给相应的异步函数**。 Within an async function, await statements cause the function to yield control to the runtime while an asynchronous I/O operation is performed. Executing an await statement yields control to the runtime. It puts the coroutine into a queue to be woken at some later time, when the I/O operation has completed. If another coroutine is ready to execute, then the runtime schedules the yielding function to wake-up once the I/O completes, and control passes to that other coroutine. Otherwise, runtime blocks until either this, or some other, I/O operation becomes ready, then passes control back to the corresponding async function.

# async & await编程模型：Programming models

```rust
fn read_exact<T: Read>(input: &mut T, buf: &mut [u8]) -> Result<(), std::io::Error> {
     let mut cursor = 0;
    while cursor < buf.len() {
        cursor += input.read(&mut buf[cursor..])?;
    }
}

async fn read_exact<T: AsyncRead>(input: &mut T, buf: &mut [u8]) -> Result<(), std::io::Error> { 
    let mut cursor = 0;
    while cursor < buf.len() {
        cursor += input.read(&mut buf[cursor..]).await?;
    }
}
```

- 形成的异步代码应遵循同步（阻塞）代码的结构。•	Resulting asynchronous code should follow structure of synchronous (blocking) code:
- 注解（async，await）表示异步性，上下文切换点•	Annotations (async, await) indicate asynchrony, context switch points
- 编译器和运行时一起工作，生成代码，当I/O操作发生时，可以在片段中执行。•	Compiler and runtime work together to generate code that can be executed in fragments when I/O operations occur

> 由此产生的异步代码遵循阻塞代码的结构。例如，如果我们看一下Rust中read_exact()函数的异步版本，我们会发现唯一的区别是async和await注释，而且输入被声明为实现AsyncRead特性的东西，而不是Read特性。除了对main的调用外，代码结构保持不变，main将异步函数包装成启动运行时间的调用。编译器和运行时一起工作，生成有效执行异步I/O操作的代码。The resulting asynchronous code follows structure of the blocking code. If we look at the async version of the read_exact() function in Rust, for example, we see that the only differences are the async and await annotations, and that the input is declared to be something that implements the AsyncRead trait, rather than the Read trait. The code structure remains unchanged, aside from the call to main that wraps the async functions into a call to start the runtime. And the compiler and runtime work together to generate code that efficiently executes the asynchronous I/O operations.

# 运行时支持：Runtime Support

- **异步代码需要运行时支持，以执行循环程序和轮询I/O源的活动**•	Asynchronous code needs runtime support to execute the coroutines and poll the I/O sources for activity
- **一个返回T类型数据的异步函数可以编译成一个返回`Impl Future<Output=T>`的普通函数**。•	An async function that returns data of type T compiles to a regular function that returns `impl Future<Output=T>`
- ![](/static/2022-05-08-17-32-46.png)
- **即，它返回一个Future值，代表一个稍后可用的值**。•	i.e., it returns a Future value that represents a value that will become available later
- **运行时不断地对Future值调用poll()，直到所有的Future值都就绪**。•	The runtime continually calls poll() on Future values until all are Ready
  - 一个Future在完成时返回Ready，在等待I/O的阻塞时返回Pending。•	A future returns Ready when complete, Pending when blocked on awaiting I/O
  - 调用tokio::run(future)启动运行时。•	Calling tokio::run(future) starts the runtime
- 在Python和JavaScript中得到很好的支持 - Rust的运行时是实验性的：https://tokio.rs/。•	Well supported in Python and JavaScript – runtime for Rust is experimental: https://tokio.rs/

> **这在Rust中是如何实现的呢？好吧，在我们之前看到的Python代码中，coroutine被实例化为一个带有next()方法的生成器对象，允许它运行并产生下一个值。Rust也做了类似的事情。异步函数被编译成维护函数状态的结构实例，并实现了一个被称为Future的trait。Future trait有一个描述返回值的成员类型，并定义了一个poll()函数，用来运行函数直到产生一个枚举的实例。而这要么是Ready，有产生的值，要么是Pending，表示异步函数正在等待I/O。如你所料，Rust和Python之间的细节有所不同，但概念是相同的**。How is this implemented in Rust? Well, in the Python code we saw earlier, the coroutine was instantiated as a generator object with a next() method that allowed it to run and yield the next value. Rust does something similar. The async functions are compiled into instances of structs that maintain the function state, and that implement a trait known as Future. The Future trait has a member type that describes the return value, and defines a poll() function that runs the function until it yields an instance of an enum. And that’s either Ready, with the yielded value, or Pending to indicate that the async function is waiting for I/O. The details differ between Rust and Python, as you might expect, but the concepts are the same.

# 3异步设计模式：Design Patterns for Asynchronous Code

讲座的最后部分讨论了如何构造和组成异步函数，并回顾了在异步函数中避免阻塞操作和长时间运行计算的必要性。讨论了异步编程模型的好处和问题。

# ====================

# 组合Future:Compose Future values

- 异步函数应该是小的，范围有限的•	async functions should be small, limited scope
- 执行一个明确定义的单一任务。•	Perform a single well-defined task:
  - 读取并解析一个文件•	Read and parse a file
  - 读取、处理和响应一个网络请求•	Read, process, and respond to a network request

- Rust提供了可以组合Future值的组合器，以产生一个新的Future。•	Rust provides combinators that can combine Future values, to produce a new Future:
  - for_each(), and_then(), read_exact(), select()
  - 可以简化异步函数的组合——但也可以混淆 •	Can ease composition of asynchronous functions – but can also obfuscate

> 在编写异步函数时，目标应该是使这些函数尽可能小，范围尽可能有限。一个异步函数应该执行一个单一的、定义明确的任务。例如，它应该读取并解析一个文件，或者它应该读取、解析、处理并响应一个网络请求。如果函数是以这种方式结构的，那么它们往往是相当直接的，而且是以一种相当自然的风格来编写的，并且可以很直接地进行组合。So in writing async functions, I think the goal should be to make these functions as small, and as limited scope, as possible. An async function should perform a single, well-defined task. It should read and parse a file, or it should read, parse, process, and respond to a network request, for example. And it functions are structured in this way, they tend to be fairly straightforward and written a fairly natural style, and compose pretty straightforwardly. 
> Rust异步库提供了一些组合器函数，可以帮助组合Future值。这可以帮助来组合Future值并产生一个新的值。还有一些函数，如read_exact()，允许它读取一个精确的字节数，如select()，允许它响应不同的Futures，这些Futures是同时运行的，还有for_each()和and_then()等函数，可以缓解异步函数的组合。The Rust async libraries provide some combinator functions that can help compose futures. That can help come combine future values and produce a new value. And there are functions, such as read_exact(), which allow it to read a an exact number of bytes, such as select() to allow it to respond to different Futures which are operating concurrently, and functions such as for_each() and and_then(), which can ease the composition of the asynchronous functions. 
> 有时这些是有帮助的，有时它们只是混淆了代码。但是有许多函数可以与Futures一起工作，并在它们有用的情况下将其结合起来。And sometimes these are helpful, sometimes they just obfuscate the code. But there are a number of functions that can work with, and combine, Futures in the cases that they’re useful.

# 避免阻塞操作：Avoid Blocking Operations

- **异步代码在单线程上复用I/O操作**•	Asynchronous code multiplexes I/O operations on single thread
- **提供I/O操作的异步感知版本**•	Provides asynchronous aware versions of I/O operations
  - 文件I/O、网络I/O（TCP、UDP、Unix套接字）。•	File I/O, network I/O (TCP, UDP, Unix sockets)
  - ![](/static/2022-05-08-17-48-57.png)
  - 非阻塞，返回与运行时交互的future值•	Non-blocking, return Future values that interact with the runtime
- **与阻塞的I/O不能很好地互动**•	Does not interact well with blocking I/O
  - 一个在I/O上阻塞的Future将阻塞整个运行时间•	A Future that blocks on I/O will block entire runtime

- **需要程序员自律，以确保异步和阻塞I/O不在代码库中混合使用**•	Programmer discipline required to ensure asynchronous and blocking I/O are not mixed within a code base
  - 包括在库函数中，等等。•	Including within library functions, etc.

> 在编写异步代码时，有两个基本的限制，你需要注意。When writing asynchronous code, there’s two fundamental constraints that you need to be aware of. 
> 首先，由于异步代码的性质，它将多个I/O操作复用到一个线程上。而运行时必须提供所有这些不同的I/O操作的异步感知版本。它必须提供对文件的异步读写，对网络、TCP、UDP和Unix套接字的异步读写，以及对其他类型的网络协议的异步读写。在所有这些情况下，它必须提供一个非阻塞版本的I/O操作，返回一个可以与运行时交互的Future。与其本机调用操作系统提供的阻塞函数，不如将操作系统提供的底层异步I/O操作包裹起来，并将其包裹在Futures中。The first, due to the nature of asynchronous code, is that it multiplexes multiple I/O operations onto a single thread. And the runtime has to provide asynchronous-aware versions of all of these different I/O operations. It has to provide asynchronous reads and writes to files, asynchronous reads and writes to the network, TCP, UDP, and Unix sockets, and to other types of network protocols. And in all of these cases, it has to provide a non-blocking version of the I/O operation, that returns a Future that can interact with the runtime. Rather than natively calling the blocking function provided by the operating system, it has to wrap the underlying the operating system provided async I/O operations, and wrap them into Futures. 
> 而且，重要的是，它与阻塞式I/O不能很好地互动。例如，如果你调用同步版本的read()，从一个文件中读取数据，而这时会阻塞整个运行时间，因为运行时间是在单线程的上下文中运行，而底层系统并不知道Futures。And, importantly, it doesn't interact well with blocking I/O. If you call the synchronous version of read(), for example, to read from a file, and that blocks, it will block the entire runtime, because the runtime is operating within the context of a single thread, and the underlying system doesn't know about Futures. 
> 程序员必须有纪律地避免调用代码中的阻塞函数，否则整个异步运行时就会在阻塞操作完成时陷入停顿。而这在某种程度上，有可能使生态系统变得支离破碎。这意味着应该在异步环境中使用的库必须被写成使用异步函数，必须被写成调用await()，必须被写成使用I/O库的异步版本。如果任何人，任何一个库的作者，在实现这些库的时候忘记了，那么你就有可能阻塞运行时间，失去并发性，失去性能。这是一个潜在的错误来源，因为Rust编译器，Rust语言，不能捕捉这种行为。程序员需要有纪律来避免阻塞的I/O操作。The programmer has to have the discipline to avoid calling the blocking functions of the code, otherwise the entire asynchronous runtime grinds to a halt while that blocking operation completes. And this, to some extent, runs the risk of fragmenting the ecosystem. It means that libraries which are supposed to be used in an asynchronous context have to be written to use asynchronous functions, have to be written to call await(), have to be written to use the asynchronous versions of the I/O libraries. And if anyone, any of the library authors, implementing any of those libraries forgets, then you run the risk of that blocking the runtime, losing concurrency, losing the performance. And this is a potential source of bugs, because the Rust compiler, the Rust language, can't catch this sort of behaviour. The programmer is required to have the discipline to avoid the blocking I/O operations.

# 避免长期运行计算：Avoid Long-running Computations

- **Future值之间的控制传递是明确的**•	Control passing between Future values is explicit
  - await调用将控制权转回给运行时•	await calls switch control back to the runtime
  - 然后安排下一个可运行的Future. •	Next runnable Future is then scheduled
  - 一个Future如果不调用await，而是执行一些长期运行的计算，就会使其他任务陷入饥饿。•	A Future that doesn’t call await, and instead performs some long-running computation, will starve other tasks

- **程序员需要遵守纪律，为长期运行的计算生成独立的线程**•	Programmer discipline required to spawn separate threads for long-running computations
  - 通过消息传递与这些线程进行通信--这可以在Future中进行调度•	Communicate with these via message passing – that can be scheduled within a Future

> 同样，程序员必须避免长时间运行的计算。在不同的Futures之间，不同的async函数之间，控制传递是明确的。当你调用 await 来等待 I/O 操作时，就会发生控制传递。这时，下一个可运行的Future，下一个可运行的异步函数，就会被安排。但是，就像调用阻塞函数是有问题的一样，因为它会导致运行时在该函数阻塞时停止，如果你不调用await，而是进行一些长期运行的计算，运行时将不知道，无法从该计算中切换出来，它将使其他任务无法运行。如果你有一个长期运行的计算，你需要生成一个单独的线程，并明确地将消息传递给该线程，以避免其他异步计算的饥饿。Similarly, the programmer has to avoid long running computations. Control passing between different Futures, between different async functions, is explicit. Control passing happens when you call it await, to wait for an I/O operation. At that point, the next runnable Future, the next runnable async function, is scheduled. But in the same way that calling blocking functions is problematic, because it causes the runtime to stop while that function blocks, if instead you don't call await, and you perform some long running computation, the runtime won't know, won’t be able to switch away from that computation, and it will starve the other tasks from running. If you have a long running computation, you need to spawn a separate thread, and explicitly pass messages to and from that thread, to avoid starving the other asynchronous computations. 
> 再说一遍，语言和运行时并不能帮助解决这个问题。程序员需要意识到，在异步运行时的背景下，你必须避免长期运行的计算，以及阻塞的I/O。因此，我们获得了良好的性能，但我们是通过限制程序员在异步运行时的操作来获得良好的性能，并要求他们有纪律地确保他们遵循这些限制。And again, the language, the runtime, doesn't help with this. The programmer needs to be aware that you must avoid long running computations, as well as blocking I/O, in the context of an asynchronous runtime. So we're getting good performance, but we're getting good performance by limiting what the programmer can do in the async runtime, and by requiring them to have the discipline to make sure that they follow those limitations.

# 何时使用异步IO：When to use

- **async/await重构代码，在单线程上有效地复用大量的I/O操作**•	async/await restructure code to efficiently multiplex large numbers of I/O operations on a single thread
- **当每个任务都是I/O绑定时，提供了一个非常自然的编程模型**•	Gives a very natural programming model when each task is I/O bound
  - 执行异步、非阻塞、I/O的代码结构与使用阻塞I/O操作的代码非常相似•	Code to perform asynchronous, non-blocking, I/O is structured very similarly to code that uses blocking I/O operations
  - 运行时可以安排许多任务在单线程上并发运行•	Runtime can schedule many tasks can run concurrently on a single thread
  - 每个任务在很大程度上被阻塞，等待I/O--很少的开销•	Each task is largely blocked awaiting I/O – little overheads

> Async和await的使用让我们能够以一种允许我们在单个线程上有效地复用大量I/O操作的方式来构造代码。当你在执行大量的I/O操作时，这可以提供一个非常自然的编程模型。它可以让我们把执行异步非阻塞式I/O的代码结构化，使其看起来与使用阻塞式I/O的代码非常相似。这可以有效地将多个I/O操作复用到一个线程上，有效地让它们并发运行，而没有启动多个线程的开销。因此，对于I/O绑定的任务，这可以是非常非常有效的，而且非常自然。 The use of async and await lets us structure the code in a way that allows us to efficiently multiplex large numbers of I/O operations on a single thread. And this can give a very natural programming model when you're performing operations which are very heavily I/O bound. It lets us structure code, which performs asynchronous non-blocking I/O, in a way that looks very similar to code that uses blocking I/O. That can efficiently multiplex multiple I/O operations onto a single thread, efficiently allow them to run concurrently, without the overheads of starting-up multiple threads. So for I/O bound tasks, this can be very, very efficient, and very natural.

---

- **async/await重构代码，在单线程上有效地复用大量的I/O操作**•	async/await restructure code to efficiently multiplex large numbers of I/O operations on a single thread
- **有问题的是阻塞性操作**•	Problematic with blocking operations
  - 如果一个任务执行了一个阻塞操作，它就会锁定整个运行时间--所有潜在的阻塞调用都必须更新为使用异步I/O操作•	If a task performs a blocking operation, it locks the entire runtime – all potentially blocking calls must be updated to use asynchronous I/O operations
  - 有可能使库的生态系统碎片化•	Potential to fragment the library ecosystem
- **长时间运行的计算有问题**•	Problematic with long-running computations
  - 长时间运行的计算使其他任务的CPU时间不足--运行时只有在异步操作开始时才会在任务之间切换•	Long-running computations starve other tasks of CPU time – runtime only switches between tasks when an asynchronous operation is started
  - 需要插入上下文切换调用--这不就是合作式多任务的重新想象吗？•	Need to insert context switch calls – isn’t this just cooperative multitasking reimagined?
    - Windows 3.1, MacOS System 7

> 但是，如果有阻塞操作，那就有问题了，因为阻塞操作锁定了整个运行时间，而不仅仅是一个任务。这意味着所有使用阻塞调用的库都需要更新以使用这些异步I/O操作。这意味着所有的东西都必须使用异步I/O，或者人们需要建立两个版本的所有库。一些是同步的，并使用阻塞操作，一些是异步的。But. It's problematic, if there are blocking operations, because the blocking operations lock-up the entire runtime, and not just that one task. And it means all the libraries, that use blocking calls need to be updated to use these asynchronous I/O operations. And this either means everything has to use asynchronous I/O, or people need to build two versions all the libraries. Some that are synchronous, and use the blocking operations, and some which are asynchronous. 
> 当涉及到长期运行的计算时，这也是个问题。同样，正如我们所看到的，它们会饿死其他任务，因为运行时只在你调用异步操作和调用await时才会切换。因此，如果你试图将长期运行的、计算量大的函数与异步I/O函数和I/O绑定的任务混合在一起，就会出现问题，并倾向于使I/O任务陷入饥饿。这个问题对于在Windows 3.1或MacIntosh系统7上写过代码的人来说是很熟悉的，它导致了这些应用程序的真正的交互性问题，在那里，应用程序不会产生CPU，它将阻止多任务工作一段时间。It’s also problematic when it comes to long running computations. Again, as we saw, they starve the other tasks, because the runtime only switches away when you call asynchronous operations, when you call await. And so, if you're trying to mix long-running, compute-heavy, functions with asynchronous I/O functions and I/O-bound tasks, it gets to be problematic, and tends to starve to I/O tasks. And this is a problem which is familiar to anyone who wrote code on Windows 3.1 or on the MacIntosh System 7 And it led to real interactivity problems with those applications, where applications would just not yield the CPU, and it would prevent the multitasking from working for a while. 
> 我们担心，通过推广异步代码，我们只是把同样难以调试的任务饥饿问题引入下一代的应用程序。当你有很多I/O绑定的任务时，异步I/O工作得非常好。它不能很好地与计算绑定的任务混合。it's worry that by promoting asynchronous code, we're just introducing the same hard to debug problems with task starvation, into the next generation of applications. The asynchronous I/O works really well when you have a lot of I/O-bound tasks. It doesn't mix well with compute-bound tasks.

# 异步IO效率：Performance

你真的需要异步I/O吗？•	Do you really need asynchronous I/O?

- 在运行时，线程比异步函数和任务更昂贵•	Threads are more expensive than async functions and tasks in a runtime
  - 但线程并不昂贵--一台配置合理的现代机器可以运行成千上万的线程•	But threads are not that expensive – a properly configured modern machine can run thousands of threads
    - 在正常使用的情况下，Core i5笔记本上运行着约2200个线程，这些幻灯片是在Core i5上准备的。•	~2,200 threads running on the Core i5 laptop these slides were prepared on, in normal use
    - Varnish网络缓存（https://varnish-cache.org）。"通常情况下，最少需要500到1000个线程"，但他们 "很少建议运行超过5000个线程"•	Varnish web cache (https://varnish-cache.org): “it’s common to operate with 500 to 1000 threads minimum” but they “rarely recommend running with more than 5000 threads”
    - 除非你在做一些非常不寻常的事情，否则你可以直接生成一个线程，或者使用预先配置好的线程池，并执行阻塞式I/O--使用通道进行通信，即使这意味着生成成千上万的线程。•	Unless you’re doing something very unusual you can likely just spawn a thread, or use a pre- configured thread pool, and perform blocking I/O – and communicate using channels, even if this means spawning thousands of threads
- 异步I/O可以带来性能上的好处•	Asynchronous I/O can give a performance benefit
  - 但这种性能优势通常是很小的•	But this performance benefit will usually be small
  - 选择异步编程是因为你喜欢这种编程风格，接受它通常不会显著提高性能。•	Choose asynchronous programming because you prefer the programming style, accepting that it will often not significantly improve performance

> 而且，在某种程度上，我想知道我们是否真的需要异步I/O？我们是否真的需要这样的性能？当然，在运行时，线程比异步函数和异步任务更昂贵，这是事实。但线程并不那么昂贵。一台配置合理的现代机器可以运行很多很多的线程，没有任何大的困难。例如，我正在录制这个讲座的笔记本电脑，这是一台配备Core i5处理器的低端MacBook Air，在正常的日常使用中可以运行大约2200个线程。如果你查阅一下文档，例如，Varnish网络缓存，这是一个在数据中心相当流行的缓存网络代理，文档中说，通常至少要配置500或1000个线程，但他们很少建议运行超过5000个线程。而且，我认为，除非你在做一些非常非常不寻常的事情，否则你很可能只需生成一个线程，或使用一个预先配置的线程池，并执行阻塞式I/O，只是使用通道进行通信，性能就会很好。即使这意味着生成成千上万的线程。现代服务器可以同时运行数千或数万个线程，没有任何大的困难。And, to some extent, I wonder whether we really need the asynchronous I/O? Whether we really need this for performance? It's certainly true that threads are more expensive than async functions, and async tasks, in the runtime. But threads are not that expensive. A properly configured modern machine can run many, many thousands of threads, without any great difficulty. The laptop I’m recording this lecture on, for example, which is a low-end MacBook Air with a Core i5 processor, is running about 2200 threads in normal everyday use. And If you look up the documentation for, for example, the Varnish web cache, which is a caching web proxy that’s quite popular in data centres, the documentation says it's common to configure this with 500 or 1000 threads, at minimum, but they rarely recommend running more than 5000 threads. And, I think, unless you're doing something very, very unusual, it's likely you can just spawn a thread, or use a pre-configured thread pool, and perform blocking I/O, and just communicate using channels, and the performance will be just fine. Even if this means spawning up thousands of threads. Modern servers can run thousands, or tens of thousands, of simultaneous threads without any great difficulty. 
> 现在的线程并不昂贵。而异步I/O可以带来性能上的好处。但这种性能优势通常不是那么大。所以我的建议是，选择异步编程是因为你喜欢这种编程风格，如果你喜欢的话。但不要因为性能原因而选择它，除非你真的确定它能提高性能。线程并不像你想象的那么昂贵。Threading is not that expensive these days. And asynchronous I/O can give a performance benefit. But that performance benefit is usually not that great. So my recommendation is, choose asynchronous programming because you prefer the programming style, if you like. But don't choose it for performance reasons, unless you really sure that it will improve performance. Threading is not as expensive as you think.

# ====================

至此，我们结束了对协程和异步I/O的讨论。正如我们所看到的，阻塞式I/O可能是有问题的。它有问题是因为它迫使你写多线程代码，而多线程代码被认为是高开销的。或者它有问题是因为你必须把你的代码构造成一个选择循环。而在这两种情况下，都需要对代码进行重组。Coroutines和异步代码的使用，在最好的情况下，可以给你一个I/O重度代码的结构，它允许你执行异步I/O操作，而不需要极大地重组代码，只是涉及一些少量的注释，并切换到使用运行时的异步版本。而且，在这些情况下，这是一个非常自然的编程模型，而且效果非常好。不过，它确实有将生态系统分割成异步感知和非异步感知函数的风险，而且我认为它有可能引入难以发现的阻塞代码和占用CPU的代码的错误。这样做值得吗？我不知道，也许吧。在某些情况下，它提供了非常自然的代码，在某些情况下提供了非常高性能的代码，我认为在这些情况下它可以很好地工作。在其他情况下，在其他应用中，多线程的、阻塞的、版本的代码写起来同样自然，而且还能很好地扩展。不同的应用有不同的要求，我鼓励大家多做实验，而不是直接使用异步函数和await。在下一讲中，我们将继续讨论并发性问题，而不是讨论安全问题。 And that concludes our discussion of coroutines and asynchronous I/O. As we've seen, blocking I/O can be problematic. It's problematic because it forces you to write multi-threaded code, which has a reputation as being high overhead. Or it's problematic because you have to structure your code as a select loop. And in both of these cases there's a restructuring of the code needed. The use of coroutines and asynchronous code, in the best cases, can give you a structure for I/O-heavy code, which allows you to perform asynchronous I/O operations without greatly restructuring the code, and just involves some small number of annotations, and switching to use the async version of the runtime. And, in those cases it's quite a natural programming model, and it works very well. It does, though, run the risk of fragmenting the ecosystem into async aware, and non-async aware functions, and I think it runs the risk of introducing hard to find bugs with blocking code, and with CPU hogging code. Is it worth it? I don't know, maybe. It gives very natural code in some cases, and very high performance code in some cases, I think it can work very well in those cases. In other cases, in other applications, the multi-threaded, blocking, version of the code is just as natural to write, and also scales very well. Different applications have different requirements, and I would encourage experimentation, rather than just diving straight in to using asynchronous functions and await. In the next lecture, we’ll move on, and instead of talking about concurrency, we'll move on to talk about security.