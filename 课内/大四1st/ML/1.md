# Content

* [Content](#content)
* [Data](#data)
* [Supervised Learning](#supervised-learning)
  * [例子](#例子)
* [Unsupervised Learning](#unsupervised-learning)
* [监督学习例子](#监督学习例子)
* [假设：Assumptions](#假设assumptions)
* [Term定义：Definitions](#term定义definitions)
* [线性模型：Linear Model](#线性模型linear-model)
* [How good is a particular w0, w1](#how-good-is-a-particular-w0-w1)
* [Loss](#loss)
* [损失函数：Squared Loss](#损失函数squared-loss)
  * [Average squared loss](#average-squared-loss)
* [优化问题：Optimisation Problem](#优化问题optimisation-problem)
  * [average squared loss in 1D](#average-squared-loss-in-1d)
  * [naive method寻找最优配置](#naive-method寻找最优配置)
  * [梯度下降求最优配置：Gradient Descent](#梯度下降求最优配置gradient-descent)
* [解出模型](#解出模型)
* [回顾假设](#回顾假设)
* [=======](#)
* [补充：到底如何找到w0,w1](#补充到底如何找到w0w1)
* [求maxima, minima](#求maxima-minima)
* [如何判断是min还是max](#如何判断是min还是max)
* [偏导数：多变量](#偏导数多变量)
* [损失函数w0, w1](#损失函数w0-w1)
  * [求w0](#求w0)
  * [求w1](#求w1)
  * [代入](#代入)

# Data

* Machine learning starts with data.
* Observations of objects:
  * Observations of people (preferences, health, etc)
  * Observations of the world (images, sounds, etc)
* Can we find similar objects?
* Can we make predictions about objects?
* Can we learn something about the objects?
* Can we group the objects?

# Supervised Learning

Regression 回归

![](/static/2021-10-03-19-37-52.png)

Learning a continuous function from a set of examples. 从一组示例中学习连续函数。

* Predicting stock prices (x might be time or some other variable of interest). 预测股票价格（x 可能是时间或其他一些感兴趣的变量）

---

Classification 分类

Learning a rule that can separate objects of different types from one another. 学习一种可以将不同类型的对象彼此分开的规则。

* Disease diagnosis, spam email detection. 疾病诊断，垃圾邮件检测。

## 例子

![](/static/2021-10-03-19-39-13.png)
![](/static/2021-10-03-19-39-22.png)

# Unsupervised Learning

Clustering 聚类

![](/static/2021-10-03-19-40-26.png)
![](/static/2021-10-03-19-40-31.png)

Finding groups of similar objects 查找相似对象的组

* People with similar ‘taste’, genes with similar function. 具有相似“品味”的人，具有相似功能的基因。

---

Projection 投影

![](/static/2021-10-03-19-40-59.png)
![](/static/2021-10-03-19-41-07.png)

# 监督学习例子

![](/static/2021-10-03-19-57-19.png)
![](/static/2021-10-03-19-57-38.png)

* 预测2012winning time

# 假设：Assumptions

![](/static/2021-10-03-20-02-32.png)

* 存在关系，能够预测

![](/static/2021-10-03-20-02-54.png)

* 存在线性关系

![](/static/2021-10-03-20-03-06.png)

* Past & future都可以预测

![](/static/2021-10-03-20-04-04.png)

# Term定义：Definitions

![](/static/2021-10-03-20-06-05.png)

* 特征 & 目标值
* 特征 & 目标值可以用变量表示

![](/static/2021-10-03-20-07-03.png)

* 线性模型，输入特征x，map目标t
* 如找到了最佳模型f(x)，令x=2012则可预测

![](/static/2021-10-03-20-09-31.png)

* 训练集，训练模型f(x)

# 线性模型：Linear Model

![](/static/2021-10-03-20-11-48.png)

* w0, w1模型参数
  * w1 - slope
* w0,w1决定要拟合模型(line)的属性

---

![](/static/2021-10-03-20-14-19.png)

* 通过训练集帮助找到w0, w1
  * feeding model

# How good is a particular w0, w1

![](/static/2021-10-03-20-18-07.png)

* search for the best w0, w1 pair
  * 能最佳拟合训练集数据的模型

# Loss

![](/static/2021-10-03-20-22-41.png)
![](/static/2021-10-03-20-23-15.png)
![](/static/2021-10-03-20-23-47.png)
![](/static/2021-10-03-20-26-27.png)

* 真值 - 模型预测值
  * 差异 -> error term
* **通常对这个error term进行平方，反映针对训练集，某模型，how badly we model**

# 损失函数：Squared Loss

针对w0, w1.任意点的损失函数

![](/static/2021-10-03-20-30-30.png)

* 因为**训练集固定**，通常可简化为 L=(w0;w1)
  * 只需要改变w0,w1观察loss
* 损失Ln越低，拟合的越好
  * the closer the line at xn passes to tn

## Average squared loss

想拟合训练集中所有的点, 通常针对训练集中每个点计算损失平方

![](/static/2021-10-03-20-55-45.png)
![](/static/2021-10-03-20-55-52.png)
![](/static/2021-10-03-20-56-31.png)

* 利用 average squared loss，可以明显对比出哪个模型更好
  * 训练集相同情况下，改变(w0, w1)

# 优化问题：Optimisation Problem

不仅仅是search for w0,w1，想找到最佳的w0,w1

* 优化问题：利用平均平方损失，找到最佳w0, w1配置 (使得average squared loss最小化)

![](/static/2021-10-03-21-01-39.png)

## average squared loss in 1D

![](/static/2021-10-03-21-05-18.png)
![](/static/2021-10-03-21-05-32.png)

## naive method寻找最优配置

![](/static/2021-10-03-21-30-06.png)

* 给定所有可能w0,w1输入配置。形成matrix
  * 可以用contour plot可视化
* idea: 就是把w1,w0输入全给了，然后计算每对的loss，找到最小的loss - 最优配置
  * 希望最佳配置是在给定的w0,w1配置中
  * 缺陷：当配置增多，，效率很低，得一个个找
  * 且最优配置也不一定在给定的输入配置中
* 图例为部分zoom in
  * 每个line代表一个特定L平均平方损失值
  * 中间那个是w0,w1全局中最小的点
    * 最小loss

## 梯度下降求最优配置：Gradient Descent

![](/static/2021-10-03-21-38-02.png)
![](/static/2021-10-03-21-42-45.png)

* 通过updating formula，**让梯度降到0（达到min点**
  * 0的时候更新步骤收敛，停止
* Leanring rate - 控制梯度下降速率

![](/static/2021-10-03-21-53-33.png)

* w0,w1同事选取初始guess值，然后梯度下降
  * 一步步找到最小loss

# 解出模型

naive method或者梯度下降解出w0, w1

![](/static/2021-10-03-21-57-30.png)
![](/static/2021-10-03-21-57-38.png)

* 思考为啥w1非常接近0，，因为按理来说非常接近0很难预测趋势【近似水平直线】

# 回顾假设

![](/static/2021-10-03-22-07-02.png)
![](/static/2021-10-03-22-07-33.png)
![](/static/2021-10-03-22-08-12.png)
![](/static/2021-10-03-22-08-33.png)
![](/static/2021-10-03-22-08-43.png)

* 取决于要预测的具体问题（问题的限制）

# =======

# 补充：到底如何找到w0,w1

# 求maxima, minima

![](/static/2021-10-03-22-18-06.png)

* 一阶导数

# 如何判断是min还是max

直到z=3时，梯度0，，到底是max还是min？

![](/static/2021-10-03-22-26-22.png)

* 二阶导数，斜率自己本身是如何变化的

# 偏导数：多变量

![](/static/2021-10-03-22-28-41.png)
![](/static/2021-10-03-22-30-50.png)

* 要两个导数都为0
  * 然后解出y,z
* 根据二阶导，判断是min, max

# 损失函数w0, w1

![](/static/2021-10-03-22-33-13.png)
![](/static/2021-10-03-22-38-24.png)
![](/static/2021-10-03-22-45-52.png)

* 解出w0, w1使得L最小。需要偏导（梯度），然后令两个导数=0

## 求w0

主要是表示w0与w1的关系

![](/static/2021-10-03-22-57-10.png)

* w0, w1都与N无关，可以提出来
* 右边两个是average target value & average attribute value

## 求w1

![](/static/2021-10-03-22-59-26.png)

## 代入

综合两个关系表达，可以写出如何求w1（从而最后解出w0

![](/static/2021-10-03-23-01-05.png)

代入训练集数据，计算出w1,w0

![](/static/2021-10-03-23-02-26.png)