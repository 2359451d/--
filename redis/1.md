# Content

[参考](https://blog.csdn.net/u011863024/article/details/107476187)

* [Content](#content)
* [是什么](#是什么)
* [作用](#作用)
* [启动后杂项基础知识](#启动后杂项基础知识)
* [Redis的五大数据类型](#redis的五大数据类型)
* [Key：关键字](#key关键字)
* [String](#string)
* [List](#list)
* [Set](#set)
* [Hash](#hash)
* [ZSet](#zset)
* [redis.conf](#redisconf)
* [持久化 - RDB](#持久化---rdb)
  * [触发条件](#触发条件)
  * [禁用](#禁用)
  * [如何恢复](#如何恢复)
  * [优势与劣势](#优势与劣势)
  * [如何停止](#如何停止)
  * [summary](#summary)
* [持久化 - AOF](#持久化---aof)
  * [AOF配置](#aof配置)
  * [AOF启动/修复/恢复](#aof启动修复恢复)
  * [rewrite](#rewrite)
  * [优势与劣势](#优势与劣势-1)
  * [summary](#summary-1)
* [AOF vs RDB](#aof-vs-rdb)
* [事务](#事务)
  * [常用命令](#常用命令)
  * [案例](#案例)
  * [watch监控](#watch监控)
    * [悲观锁/乐观锁/CAS(Check And Set)](#悲观锁乐观锁cascheck-and-set)
    * [案例](#案例-1)
  * [unwatch](#unwatch)
    * [summary](#summary-2)
  * [事务3阶段](#事务3阶段)
  * [事务3特性](#事务3特性)
* [消息订阅发布](#消息订阅发布)
  * [常用命令](#常用命令-1)
* [主从复制](#主从复制)
  * [使用](#使用)
  * [一主二从案例](#一主二从案例)
  * [去中心化](#去中心化)
  * [SLAVEOF no one](#slaveof-no-one)
  * [复制原理](#复制原理)
  * [哨兵模式(sentinel)](#哨兵模式sentinel)
    * [使用](#使用-1)
    * [复制的缺点](#复制的缺点)
* [Jedis](#jedis)
  * [常用API](#常用api)
  * [事务](#事务-1)
  * [主从复制](#主从复制-1)
* [JedisPool](#jedispool)
  * [配置](#配置)

# 是什么

> **Redis:REmote DIctionary Server(远程字典服务器**)是完全开源免费的，用C语言编写的，遵守BSD协议，是一个高性能的(key/value)分布式内存数据库，基于内存运行 并支持持久化的NoSQL数据库，是当前最热门的NoSql数据库之一，也被人们称为数据结构服务器

Redis 与其他 key - value 缓存产品有以下三个特点：

* Redis**支持数据的持久化**，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用
* Redis**不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储**
* Redis**支持数据的备份**，即master-slave模式的数据备份

# 作用

* 内存存储和持久化：redis支持异步将内存中的数据写到硬盘上，同时不影响继续服务
* 取最新N个数据的操作，如：可以将最新的10条评论的ID放在Redis的List集合里面
* 模拟类似于HttpSession这种需要设定过期时间的功能
* 发布、订阅消息系统
* 定时器、计数器

# 启动后杂项基础知识

`redis-benchmark`

* 测试redis在机器运行的效能

单进程

![](/static/2021-08-19-15-34-23.png)

---

默认16个数据库，类似数组下表从零开始，初始默认使用零号库，可在配置文件配置

* `select`切换数据库

`dbsize`查看当前数据库key数量

* `keys *`列出所有key
  * 支持ant风格

`flushdb`清空当前库

`flushall`通杀全部库

# Redis的五大数据类型

* **String**（字符串）
  * string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。
  * string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。
  * string类型是Redis最基本的数据类型，一个redis中字符串value最多可以是512M
* **Hash**
  * Redis hash 是一个键值对集合。
  * Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。
  * 类似Java里面的Map<String,Object>
* **List**
  * Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。
  * 它的底层实际是个**链表**
* **Set**
  * Redis的Set是string类型的无序集合。它是通过HashTable实现实现的
* **Zset**
  * Redis zset 和 set 一样也是string类型元素的集合，且不允许重复的成员。
  * 不同的是每个元素都会关联一个double类型的分数。
  * redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的，但分数(score)却可以重复

# Key：关键字

* `keys *`
  * 支持ant风格
* `set key`
  * 存在会覆盖
* `get key`
* `type key`
  * 查看类型
* `EXISTS key`
  * 检查给定 key 是否存在
* `MOVE key db`
  * 将当前数据库的 key 移动到给定的数据库 db 当中
* `EXPIRE key seconds`
  * 为给定 key 设置过期时间，以秒计
  * 过期移除
* `TTL key`
  * 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)
* `DEL key`
  * 该命令用于在 key 存在时删除 key

# String

单值单value

* `APPEND key value`
  * 如果 key 已经存在并且是一个字符串， APPEND 命令将指定的 value 追加到该 key 原来值（value）的末尾
* `STRLEN key`
  * 返回 key 所储存的字符串值的长度
* `INCR key`
  * 将 key 中储存的数字值增一
* `INCRBY key increment`
  * 将 key 所储存的值加上给定的增量值（increment）
* `DECR key`
  * 将 key 中储存的数字值减一
* `DECRBY key decrement`
  * key 所储存的值减去给定的减量值（decrement）
* `GETRANGE key start end`
  * 返回 key 中字符串值的子字符
* `SETRANGE key offset value`
  * 用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始
* `SETEX key seconds value`
  * 将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)
* `SETNX key value`
  * 只有在 **key 不存在时设置 key 的值**（避免覆盖）
* `MSET key value [key value …]`
  * 同时设置一个或多个 key-value 对
* `MSETNX key value [key value …]`
  * 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在
* `MGET key1 [key2…]`
  * 获取所有(一个或多个)给定 key 的值

# List

单值多value

* 它是一个字符串链表，left、right都可以插入添加；
* 如果键不存在，创建新的链表；
* 如果键已存在，新增内容；
* 如果值全移除，对应的键也就消失了。
* 链表的操作无论是头和尾效率都极高，但假如是对中间元素进行操作，效率就很惨淡了。

---

* `LPUSH key value1 [value2]`
  * 将一个或多个值插入到列表头部
* `RPUSH key value1 [value2]`
  * 在列表中添加一个或多个值
* `LRANGE key start stop`
  * 获取列表指定范围内的元素
* `LPOP key`
  * 移出并获取列表的第一个元素
* `RPOP key`
  * 移除列表的最后一个元素，返回值为移除的元素
* `LINDEX key index`
  * 通过索引获取列表中的元素
* `LLEN key`
  * 获取列表长度
* `LREM key count value`
  * 移除列表元素
* `LTRIM key start stop`
  * 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除
* `RPOPLPUSH source destination`
  * 移除列表的第一个元素，并将该元素添加到另一个列表（尾部）并返回
* `LINSERT key BEFORE/AFTER pivot value	`
  * 在列表的某个元素前或者后插入元素
* `LSET key index value`
  * 通过索引设置列表元素的值

# Set

单值多value

* `SADD key member1 [member2]`
  * 向集合添加一个或多个成员
  * 会去重
* `SISMEMBER key member`
  * 判断 member 元素是否是集合 key 的成员
* `SCARD key`
  * 获取集合的成员数
* `SREM key member1 [member2]`
  * 移除集合中一个或多个成员
* `SMEMBERS key`
  * 返回集合中的所有成员
* `SRANDMEMBER key [count]	`
  * 返回集合中一个或多个随机数
* `SPOP key`
  * 移除并返回集合中的一个**随机元素**
* `SMOVE source destination member`
  * 将 member 元素从 source 集合移动到 destination 集合
* `SDIFF key1 [key2]`
  * 返回给定所有集合的差集
  * 在key1，不在key2的
* `SINTER key1 [key2]`
  * 返回给定所有集合的交集
* `SUNION key1 [key2]`
  * 返回所有给定集合的并集

# Hash

KV模式不变，**但V是一个键值对**

* 如`hset user id 11`,
  * `key` - `user`
  * `value` - `id 11`

---

* `HSET key field value`
  * 将哈希表 key 中的字段 field 的值设为 value
* `HMSET key field1 value1 [field2 value2 ]`
  * 同时将多个 field-value (域-值)对设置到哈希表 key 中
* `HSETNX key field value`
  * 只有在字段 field 不存在时，设置哈希表字段的值
* `HGET key field`
  * 获取存储在哈希表中指定字段的值
* `HMGET key field1 [field2]`
  * 获取所有给定字段的值
* `HGETALL key`
  * 获取在哈希表中指定 key 的所有字段和值
* `HDEL key field1 [field2]`
  * 删除一个或多个哈希表字段
* `HLEN key`
  * 获取哈希表中字段的数量
* `HEXISTS key field`
  * 查看哈希表 key 中，指定的字段是否存在
* `HKEYS key`
  * 获取所有哈希表中的字段
* `HVALS key`
  * 获取哈希表中所有值
* `HINCRBY key field increment`
  * 为哈希表 key 中的指定字段的整数值加上增量 increment
* `HINCRBYFLOAT key field increment`
  * 为哈希表 key 中的指定字段的浮点数值加上增量 increment

# ZSet

在set基础上，加一个score值。 之前set是k1 v1 v2 v3， 现在zset是k1 score1 v1 score2 v2

![](/static/2021-08-19-21-26-21.png)

* `ZADD key score1 member1 [score2 member2]`
  * 向有序集合添加一个或多个成员，或者更新已存在成员的分数
* `ZRANGE key start stop [WITHSCORES]`
  * 通过索引区间返回有序集合指定区间内的成员
* `ZREVRANGE key start stop [WITHSCORES]`
  * 返回有序集中指定区间内的成员，通过索引，分数从高到低
* `ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT]`
  * 通过分数返回有序集合指定区间内的成员
  * `(`不包含，
    * 如`ZRANGEBYSCORE zset01 (60 (90` >60 , <90
  * Limit 作用是返回限制
    * `limit 开始下标步 截取长度`
    * ![](/static/2021-08-19-21-34-30.png)
* `ZREVRANGEBYSCORE key max min [WITHSCORES]`
  * 返回有序集中指定分数区间内的成员，分数从高到低排序
* `ZREM key member [member …]`
  * 移除有序集合中的一个或多个成员
* `ZCARD key`
  * 获取有序集合的成员数
* `ZCOUNT key min max`
  * 获取有序集合的成员数
* `ZRANK key member`
  * 返回有序集合中指定成员的索引
* `ZSCORE key member`
  * 返回有序集中，成员的分数值

# redis.conf

Redis 的配置文件位于 Redis 安装目录下，文件名为 `redis.conf`(Windows 名为 redis.windows.conf)。

你可以通过 CONFIG 命令查看或设置配置项

* `redis 127.0.0.1:6379> CONFIG GET/SET CONFIG_SETTING_NAME`

---

UNITS

![](/static/2021-08-20-15-16-44.png)

INCLUDE

![](/static/2021-08-20-15-16-08.png)

GENERAL

![](/static/2021-08-20-15-17-17.png)
![](/static/2021-08-20-15-17-43.png)

* `loglevel notice`
  * 指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默认为 notice
* `requirepass foobared`
  * 设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 **AUTH 命令提供密码**，默认关闭

LIMITS

![](/static/2021-08-20-16-24-54.png)

* `maxclients 128`
  * 设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数为 Redis 进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis 会关闭新的连接并向客户端返回 max number of clients reached 错误信息
* `maxmemory <bytes>`
  * 指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后，Redis 会先尝试清除已到期或即将到期的 Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key 存放内存，Value 会存放在 swap 区
* `maxmemory-policy`
  * ![](/static/2021-08-20-16-29-09.png)
* `maxmemory-samples`
  * ![](/static/2021-08-20-16-30-22.png)
  * 这个采样5个是从每个数据库中随机抽取5个数据，然后再利用LRU算法算出被丢弃的数据

# 持久化 - RDB

RDB（Redis DataBase）

是什么

* **在指定的时间间隔内将内存中的数据集快照写入磁盘**，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里
* Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到 一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 **整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能**。如果需要进行大规模数据的恢复，且对于数据恢复的完整性（**对数据精度要求不高**）不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失
  * Fork - Fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等） 数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程
* rdb 保存的是dump.rdb文件
* 相关配置在配置文件的位置 - 在redis.conf搜寻### SNAPSHOTTING ###

## 触发条件

默认配置

![](/static/2021-08-20-17-08-27.png)

---

* 配置文件中默认的快照配置`dbfilename dump.rdb`
  * 冷拷贝后重新使用
    * 可以cp dump.rdb dump_new.rdb
* 命令save或者是bgsave
  * Save：save时只管保存，其它不管，全部阻塞
    * `save <seconds> <changes>`配置
    * 或直接使用`save`指令，迅速备份
  * BGSAVE：Redis会在**后台异步（备份时还能做别的操作**）进行快照操作， 快照同时还可以响应客户端请求。可以通过lastsave 命令获取最后一次成功执行快照的时间
* 执行`flushall/shutdown`命令，也会产生dump.rdb文件，但里面是空的，无意义

---

其他相关配置

![](/static/2021-08-20-17-23-12.png)
![](/static/2021-08-20-17-23-19.png)
![](/static/2021-08-20-17-25-57.png)
![](/static/2021-08-20-17-26-27.png)

## 禁用

![](/static/2021-08-20-17-20-18.png)

不使用save指令

## 如何恢复

* 将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可
  * `cp dump_bk.rdb dump.rdb`
* `CONFIG GET dir`获取目录

## 优势与劣势

优势

* 适合大规模的数据恢复
* 对数据完整性和一致性要求不高

劣势

* 在一定间隔时间做一次备份，所以如果redis意外down掉的话，就 **会丢失最后一次快照后的所有修改**
* Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑

## 如何停止

动态所有停止RDB保存规则的方法：`redis-cli config set save ""`

## summary

![](/static/2021-08-20-17-31-37.png)

* RDB是一个非常紧凑的文件。
* RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他I0操作，所以RDB持久化方式可以最大化redis的性能。
* 与AOF相比，在恢复大的数据集的时候，RDB方式会更快一一些。
* 数据丢失风险大。
* RDB需要经常fork子进程来保存数据集到硬盘上，当数据集比较大的时候fork的过程是非常耗时的吗，可能会导致Redis在一些毫秒级不能回应客户端请求。
  * 在fork的时候，创建一个子进程。如果需要rdb的快照文件很大，可能会造成堵塞 卡顿。如果在这个时候 有大量的请求进入 可能会造成宕机

# 持久化 - AOF

AOF（Append Only File）

**以日志的形式来记录每个写操作**，将Redis执行过的所有写指令记录下来(读操作不记录)， 只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis 重启的话就**根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作**

* `FLUSHALL`也是写

rdb和aof可以同时存在，先找aof

## AOF配置

![](/static/2021-08-20-17-39-11.png)
![](/static/2021-08-20-19-53-30.png)

* 相关配置在配置文件的位置 - 在redis.conf搜寻### APPEND ONLY MODE ###
* aof保存的是`appendonly.aof`文件（在配置文件可修改文件名）

`appendonly`

![](/static/2021-08-20-19-53-51.png)

`appendfilename`

`Appendsync`

![](/static/2021-08-20-19-54-26.png)
![](/static/2021-08-20-19-54-36.png)

其他(重写相关)

![](/static/2021-08-20-19-56-45.png)

* Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发

## AOF启动/修复/恢复

![](/static/2021-08-20-17-40-48.png)

正常恢复

* 启动：设置Yes
  * 修改默认的`appendonly no`，改为yes
* 将有数据的aof文件复制一份保存到对应目录(config get dir)
* 恢复：重启redis然后重新加载

异常恢复

* 启动：设置Yes
  * 修改默认的appendonly no，改为yes
* 备份被写坏的AOF文件
* 修复：
  * `Redis-check-aof --fix`进行修复
    * 会自动修复语法不正确的部分
* 恢复：重启redis然后重新加载

## rewrite

重写相关配置
![](/static/2021-08-20-19-56-45.png)

---

是什么：

* **AOF采用文件追加方式，文件会越来越大**。为避免出现此种情况，新增了重写机制， 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩， 只保留可以恢复数据的最小指令集。可以使用命令bgrewriteaof

重写原理

* AOF文件持续增长而过大时，**会fork出一条新进程来将文件重写(也是先写临时文件最后再rename**)， 遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件， 而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似

触发机制

![](/static/2021-08-20-20-05-06.png)

* Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发

## 优势与劣势

优势

* 每修改同步：appendfsync always 同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好
* 每秒同步：appendfsync everysec 异步操作，每秒记录 如果一秒内宕机，有数据丢失
不同步：appendfsync no 从不同步

劣势

* 相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb
* Aof运行效率要慢于rdb,每秒同步策略效率较好，不同步效率和rdb相同

## summary

![](/static/2021-08-20-20-09-05.png)

* AOF文件时一个只进行追加的日志文件
* Redis可以在AOF文件体积变得过大时，自动地在后台对AOF进行重写
* AOF文件有序地保存了对数据库执行的所有写入操作，这些写入操作以Redis协议的格式保存，因此AOF文件的内容非常容易被人读懂，对文件进行分析也很轻松
* 对于相同的数据集来说，AOF文件的体积通常要大于RDB文件的体积
* 根据所使用的fsync 策略，AOF的速度可能会慢于RDB

# AOF vs RDB

官方建议：快速恢复，对数据不敏感 - RDB

![](/static/2021-08-20-20-10-15.png)

---

同时开启两种持久化方式

![](/static/2021-08-20-20-11-04.png)

---

性能建议

![](/static/2021-08-20-20-11-39.png)

# 事务

是什么

* 可以一次执行多个命令，本质是一组命令的集合。一个事务中的所有命令都会序列化，**按顺序地串行化执行而不会被其它命令插入，不许加塞**。

能干嘛

* 一个队列中，一次性、顺序性、排他性的执行一系列命令。

## 常用命令

* `DISCARD`
  * 取消事务，放弃执行事务块内的所有命令
* `EXEC`
  * 执行所有事务块内的命令
* `MULTI`
  * 标记一个事务块的开始
* `UNWATCH`
  * 取消 WATCH 命令对所有 key 的监视
* `WATCH key [key …]`
  * 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断

## 案例

正常执行

![](/static/2021-08-21-14-16-10.png)

放弃事务

![](/static/2021-08-21-14-16-36.png)

全体连坐（事务回滚

![](/static/2021-08-21-14-17-06.png)

* 类似Java编译异常

---

![](/static/2021-08-21-14-18-28.png)

* 类似Java运行异常

---

## watch监控

WATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行

### 悲观锁/乐观锁/CAS(Check And Set)

悲观锁

* 悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，**所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁**。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

乐观锁

* 乐观锁(Optimistic Lock), 顾名思义，就是很乐观，**每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制**。乐观锁适用于多读的应用类型，这样可以提高吞吐量。
* **乐观锁策略:提交版本必须大于记录当前版本才能执行更新**
  * ?

CAS

### 案例

初始化信用卡可用余额和欠额
![](/static/2021-08-21-14-34-06.png)

无加塞篡改，先监控再开启multi， 保证两笔金额变动在同一个事务内
![](/static/2021-08-21-14-35-58.png)

有加塞篡改

![](/static/2021-08-21-14-38-29.png)

* 监控了key，如果key被修改了，后面一个事务的执行失效

## unwatch

撤销本次（所有key的）监控，重新监控

![](/static/2021-08-21-14-42-18.png)

* 一旦执行了exec之前加的监控锁都会被取消掉了（一次性）
* 如果在执行 WATCH 命令之后， EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了

### summary

* **Watch指令，类似乐观锁**，事务提交时，如果Key的值已被别的客户端改变， 比如某个list已被别的客户端push/pop过了，整个事务队列都不会被执行
* **通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化， EXEC命令执行的事务都将被放弃，同时返回Nullmulti-bulk应答以通知调用者事务执行失败**

## 事务3阶段

* 开启：以MULTI开始一个事务
* 入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面
* 执行：由EXEC命令触发事务

## 事务3特性

* **单独的隔离操作**：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。
* **没有隔离级别的概念**：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行， 也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题
* **不保证原子性（部分支持事务**）：redis同一个事务中**如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚**

不遵循传统的ACID中的AI

# 消息订阅发布

用观察者模式理解学习

**进程间的一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息**。

下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系

![](/static/2021-08-21-15-56-01.png)

当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端：

![](/static/2021-08-21-15-57-12.png)

## 常用命令

![](/static/2021-08-21-15-57-36.png)
![](/static/2021-08-21-16-01-00.png)
![](/static/2021-08-21-16-01-27.png)
![](/static/2021-08-21-16-02-38.png)

# 主从复制

行话：也就是我们所说的主从复制，主机数据更新后根据配置和策略， 自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主

作用

* 读写分离
* 容灾恢复

## 使用

![](/static/2021-08-21-16-07-35.png)

* 配从(库)不配主(库)
* 从库配置命令：`slaveof 主库IP 主库端口`
  * 每次与master断开之后，都需要重新连接，除非你配置进redis.conf文件（具体位置：redis.conf搜寻#### REPLICATION ####）
  * `info replication`

修改配置文件细节操作

* 拷贝多个redis.conf文件，按’`redis[port].conf`’重命名
  * ![](/static/2021-08-21-16-12-50.png)
  * ![](/static/2021-08-21-16-15-04.png)
  * 如法炮制个6381端口的
* 开启daemonize yes
* pid文件名字
* 指定端口
* log文件名字
* dump.rdb名字

## 一主二从案例

init

![](/static/2021-08-21-18-45-26.png)
![](/static/2021-08-21-19-11-06.png)

* 默认身份都是master

一主两从（`info replication`查看信息？）

![](/static/2021-08-21-19-14-03.png)

问题

* **切入点问题？slave1、slave2是从头开始复制还是从切入点开始复制?比如从k4进来，那之前的123是否也可以复制**？
  * 从头开始复制；123也可以复制
* **从机是否可以写？set可否**
  * 从机不可写，不可set，主机可写
* **主机shutdown后情况如何？从机是上位还是原地待命**
  * 从机还是原地待命（咸鱼翻身，还是咸鱼）
* **主机又回来了后，主机新增记录，从机还能否顺利复制？**
  * 能
* **其中一台从机down后情况如何？依照原有它能跟上大部队吗**
  * 不能跟上，每次与master断开之后，都需要重新连接，**除非你配置进redis.conf文件**（具体位置：redis.conf搜寻#### REPLICATION ####）

## 去中心化

* 上一个Slave可以是下一个slave的Master，Slave同样可以接收其他 slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力（奴隶的奴隶还是奴隶）
* **中途变更转向**：会清除之前的数据，重新建立拷贝最新的
* `slaveof 新主库IP 新主库端口`

79->80->81

## SLAVEOF no one

`SLAVEOF no one`

* 使当前数据库停止与其他数据库的同步，转成主数据库

## 复制原理

* slave启动成功连接到master后会发送一个sync命令
* master接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令， 在后台进程执行完毕之后，master将传送整个数据文件到slave,以完成一次完全同步
* **全量复制**：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。
* **增量复制**：Master继续将新的所有收集到的修改命令依次传给slave,完成同步
* 但是只要是重新连接master，一次完全同步（全量复制)将被自动执行

## 哨兵模式(sentinel)

一组sentinel能同时监控多个master

* 反客为主的自动版，能够**后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库**

### 使用

![](/static/2021-08-21-20-14-34.png)

新建sentinel.conf文件，名字绝不能错

配置哨兵,填写内容

* `sentinel monitor <master-name> <ip> <redis-port> <quorum>`
  * `sentinel monitor 被监控数据库名字(自己起名字) 127.0.0.1 6379 1`
    * 上面最后一个数字1，**表示主机挂掉后salve投票看让谁接替成为主机，得票数多少后成为主机**（PS. 跟官网的描述有出入，下面有官方文档说明）

启动哨兵

* redis-sentinel /sentinel.conf（上述目录依照各自的实际情况配置，可能目录不同）
* ![](/static/2021-08-21-21-39-49.png)

如果之前挂了的master重启回来，会不会双master冲突？

* 答： 不会，原master，变成slave

### 复制的缺点

复制延时

由于所有的写操作都是先在Master上操作，然后同步更新到slave上，**所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重**

# Jedis

依赖

```xml
<dependency>
	<groupId>redis.clients</groupId>
	<artifactId>jedis</artifactId>
	<version>2.1.0</version>
</dependency>
```

测试联通

```java
package com.lun.shang;

import redis.clients.jedis.Jedis;

public class TestPing {
    public static void main(String[] args) 
    {
        Jedis jedis = new Jedis("127.0.0.1",6379);
        //输出PONG，redis连通成功
        System.out.println(jedis.ping());
    }
}
```

## 常用API

```java
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

import redis.clients.jedis.Jedis;

public class TestAPI {
	public static void main(String[] args) {

		Jedis jedis = new Jedis("127.0.0.1", 6379);
		// key
		Set<String> keys = jedis.keys("*");
		for (Iterator iterator = keys.iterator(); iterator.hasNext();) {
			String key = (String) iterator.next();
			System.out.println(key);
		}
		System.out.println("jedis.exists====>" + jedis.exists("k2"));
		System.out.println(jedis.ttl("k1"));
		
		// String
		// jedis.append("k1","myreids");
		System.out.println(jedis.get("k1"));
		jedis.set("k4", "k4_redis");
		System.out.println("----------------------------------------");
		jedis.mset("str1", "v1", "str2", "v2", "str3", "v3");
		System.out.println(jedis.mget("str1", "str2", "str3"));
		
		
		// list
		System.out.println("----------------------------------------");
		// jedis.lpush("mylist","v1","v2","v3","v4","v5");
		List<String> list = jedis.lrange("mylist", 0, -1);
		for (String element : list) {
			System.out.println(element);
		}
		
		// set
		jedis.sadd("orders", "jd001");
		jedis.sadd("orders", "jd002");
		jedis.sadd("orders", "jd003");
		Set<String> set1 = jedis.smembers("orders");
		for (Iterator iterator = set1.iterator(); iterator.hasNext();) {
			String string = (String) iterator.next();
			System.out.println(string);
		}
		jedis.srem("orders", "jd002");
		System.out.println(jedis.smembers("orders").size());
		
		// hash
		jedis.hset("hash1", "userName", "lisi");
		System.out.println(jedis.hget("hash1", "userName"));
		Map<String, String> map = new HashMap<String, String>();
		map.put("telphone", "13811814763");
		map.put("address", "atguigu");
		map.put("email", "abc@163.com");
		jedis.hmset("hash2", map);
		List<String> result = jedis.hmget("hash2", "telphone", "email");
		for (String element : result) {
			System.out.println(element);
		}
		
		// zset
		jedis.zadd("zset01", 60d, "v1");
		jedis.zadd("zset01", 70d, "v2");
		jedis.zadd("zset01", 80d, "v3");
		jedis.zadd("zset01", 90d, "v4");

		Set<String> s1 = jedis.zrange("zset01", 0, -1);
		for (Iterator iterator = s1.iterator(); iterator.hasNext();) {
			String string = (String) iterator.next();
			System.out.println(string);
		}

	}
}
```

## 事务

```java

import redis.clients.jedis.Jedis;
import redis.clients.jedis.Response;
import redis.clients.jedis.Transaction;

public class Test03 {
	public static void main(String[] args) {
        Jedis jedis = new Jedis("127.0.0.1", 6379);

        // 监控key，如果该动了事务就被放弃
        /*
            * 3 jedis.watch("serialNum"); jedis.set("serialNum","s#####################");
            * jedis.unwatch();
            */

        Transaction transaction = jedis.multi();// 被当作一个命令进行执行
        Response<String> response = transaction.get("serialNum");
        transaction.set("serialNum", "s002");
        response = transaction.get("serialNum");
        transaction.lpush("list3", "a");
        transaction.lpush("list3", "b");
        transaction.lpush("list3", "c");

        transaction.exec();
        // 2 transaction.discard();
        System.out.println("serialNum***********" + response.get());
	}
}
```

## 主从复制

* 6379,6380启动，先各自先独立
* 主写，从读

```java

import redis.clients.jedis.Jedis;

public class TestMS {
	public static void main(String[] args) {
		Jedis jedis_M = new Jedis("127.0.0.1", 6379);
		Jedis jedis_S = new Jedis("127.0.0.1", 6380);

		jedis_S.slaveof("127.0.0.1", 6379);

		jedis_M.set("class", "1122V2");

		String result = jedis_S.get("class");//可能有延迟，需再次启动才能使用
		System.out.println(result);
	}
}
```

# JedisPool

JedisPoolUtil

* **获取Jedis实例需要从JedisPool中获取**
* 用完Jedis实例需要返还给JedisPool
* 如果Jedis在使用过程中出错，则也需要还给JedisPool

```java
import redis.clients.jedis.Jedis;
import redis.clients.jedis.JedisPool;
import redis.clients.jedis.JedisPoolConfig;

public class JedisPoolUtil {
	private static volatile JedisPool jedisPool = null;

	private JedisPoolUtil() {
	}

	public static JedisPool getJedisPoolInstance() {
		if (null == jedisPool) {
			synchronized (JedisPoolUtil.class) {
				if (null == jedisPool) {
					JedisPoolConfig poolConfig = new JedisPoolConfig();
					poolConfig.setMaxActive(1000);
					poolConfig.setMaxIdle(32);
					poolConfig.setMaxWait(100 * 1000);
					poolConfig.setTestOnBorrow(true);

					jedisPool = new JedisPool(poolConfig, "127.0.0.1", 6379);
				}
			}
		}
		return jedisPool;
	}

	public static void release(JedisPool jedisPool, Jedis jedis) {
		if (null != jedis) {
			jedisPool.returnResourceObject(jedis);
		}
	}
}
```

```java
import redis.clients.jedis.Jedis;
import redis.clients.jedis.JedisPool;

public class TestPool {

	public static void main(String[] args) {
		JedisPool jedisPool = JedisPoolUtil.getJedisPoolInstance();
		JedisPool jedisPool2 = JedisPoolUtil.getJedisPoolInstance();

		System.out.println(jedisPool == jedisPool2);

		Jedis jedis = null;
		try {
			jedis = jedisPool.getResource();
			jedis.set("aa", "bb");
		} catch (Exception e) {
			e.printStackTrace();
		} finally {
			JedisPoolUtil.release(jedisPool, jedis);
		}
	}
}
```

## 配置

![](/static/2021-08-22-00-02-52.png)

其中JedisPoolConfig对一些参数的默认设置如下：

* testWhileIdle=true
* minEvictableIdleTimeMills=60000
* timeBetweenEvictionRunsMillis=30000
* numTestsPerEvictionRun=-1