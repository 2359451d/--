# Content

* [Content](#content)
* [定义](#定义)
* [概率vs非概率：Probabilistic vs non-probabilistic classifier](#概率vs非概率probabilistic-vs-non-probabilistic-classifier)
* [KNN](#knn)
* [Problems with KNN](#problems-with-knn)
* [Cross-validation for classification](#cross-validation-for-classification)
* [例子](#例子)
* [==================](#)
* [](#-1)
* [==================](#-2)
* [==================](#-3)

# 定义

![](/static/2022-05-18-14-39-44.png)

# 概率vs非概率：Probabilistic vs non-probabilistic classifier

![](/static/2022-05-18-14-44-46.png)

* 概率在错误分类的成本很高且不平衡的情况下尤其重要。Particularly important where cost of misclassification is high and imbalanced.
  * 例如，诊断：告诉一个病人他们是健康的，比告诉一个健康人他们是有病的要糟糕得多。e.g. Diagnosis: telling a diseased person they are healthy is much worse than telling a healthy person they are diseased.
* 额外的信息（概率）往往是有代价的。Extra information (probability) often comes at a cost.
  * 对于大型数据集，可能不得不采用非概率的方法。 For large datasets, might have to go with non-probabilistic.

# KNN

* I Non-probabilistic.
* I Can do binary or multi-class.
* I No ‘training’ phase.

---

![](/static/2022-05-18-15-31-41.png)
![](/static/2022-05-18-15-32-00.png)

* Choose K
*  For a test object xnew:
* Find the **K closest points** from the training set.
* Find majority class of these K neighbours.
  * (Assign **randomly** in case of a **tie**)

# Problems with KNN

![](/static/2022-05-18-16-09-24.png)
训练集分类数据不均，，K越大某类可能偏向

交叉验证选K

# Cross-validation for classification

number of mis-classifications, use K that minimises it

![](/static/2022-05-18-16-18-07.png)

# 例子

![](/static/2022-05-18-16-22-24.png)
![](/static/2022-05-18-16-22-31.png)
![](/static/2022-05-18-16-22-51.png)

# ==================

# 

# ==================
# ==================