# Content

* [Content](#content)
* [MLE&最小二乘法关联](#mle最小二乘法关联)
* [概率-分号，竖线](#概率-分号竖线)
* [方差，偏差](#方差偏差)
* [MLE](#mle)
* [We should model the errors](#we-should-model-the-errors)
* [附加错误：Additive errors](#附加错误additive-errors)
* [En as a random variable](#en-as-a-random-variable)
* [离散和连续型随机变量：Discrete and continuous RVs](#离散和连续型随机变量discrete-and-continuous-rvs)
  * [离散型随机变量：Discrete RVs](#离散型随机变量discrete-rvs)
  * [连续型随机变量：Continuous RVs](#连续型随机变量continuous-rvs)
* [联合概率和密度：Joint probabilities and densities](#联合概率和密度joint-probabilities-and-densities)
* [耦合/独立：Dependence/Independence](#耦合独立dependenceindependence)
* [条件概率：Conditioning](#条件概率conditioning)
  * [连续RV-条件](#连续rv-条件)
* [高斯模型:Gaussian model](#高斯模型gaussian-model)
* [Generating data](#generating-data)
* [随机变量t：t is a random variable too](#随机变量tt-is-a-random-variable-too)
* [似然性：Likelihood](#似然性likelihood)
* [Summary](#summary)
* [补充：如何生成随机变量的高斯概率密度模型](#补充如何生成随机变量的高斯概率密度模型)
* [===============](#)
* [似然性例子：Likelihood Example](#似然性例子likelihood-example)
* [似然性优化：Likelihood Optimisation](#似然性优化likelihood-optimisation)
  * [为什么用Log](#为什么用log)
* [最佳参数：Optimum parameters](#最佳参数optimum-parameters)
* [====================](#-1)
* [MAP](#map)
* [参数估计可信度：Confidence in parameter estimates](#参数估计可信度confidence-in-parameter-estimates)
* [期望值：Expectations](#期望值expectations)
  * [采样：Sampling](#采样sampling)
  * [离散型RV期望值](#离散型rv期望值)
  * [连续型RV期望值](#连续型rv期望值)
  * [general - 随机变量函数](#general---随机变量函数)
  * [Gaussians Distribution例子](#gaussians-distribution例子)
* [E{w} & cov{w}](#ew--covw)
* [cov{w}](#covw)
  * [例子](#例子)
* [Summary](#summary-1)
* [预测：Predictions](#预测predictions)
  * [方差：Prediction & Variance](#方差prediction--variance)
* [预测例子](#预测例子)
  * [Not complex enough model – more ‘noise’](#not-complex-enough-model--more-noise)
  * [Too complex model – parameters not well defined](#too-complex-model--parameters-not-well-defined)
* [Olympic prediction](#olympic-prediction)
* [可以基于似然性选择模型吗：Can we use likelihood to choose models?](#可以基于似然性选择模型吗can-we-use-likelihood-to-choose-models)
* [Summary](#summary-2)

# MLE&最小二乘法关联

在线性回归中，若误差服从正态分布（服从正态分布），则最小二乘法就是极大似然估计

![](/static/2021-11-05-23-00-09.png)
![](/static/2021-11-05-23-01-50.png)

# 概率-分号，竖线

![](/static/2021-11-02-05-56-09.png)
![](/static/2021-11-02-06-20-04.png)

* 后面跟概率分布的参数（变量）时，，即参数等于 时的似然是多少。

# 方差，偏差

![](/static/2021-11-03-04-55-22.png)

* bias, variance

# MLE

# We should model the errors

We know they’re there - shouldn’t ignore them.

![](/static/2021-10-21-04-31-48.png)
![](/static/2021-10-21-04-38-21.png)

# 附加错误：Additive errors

![](/static/2021-10-21-04-48-54.png)

* 需要**引入随机变量**来model errors
  * n不同，误差不同
  * 可正可负
  * 与n之间没有直接关联

# En as a random variable

episilon

![](/static/2021-10-21-04-54-28.png)

* 不知道X会取哪个值，但是知道X能取的值

# 离散和连续型随机变量：Discrete and continuous RVs

![](/static/2021-10-21-04-58-36.png)

## 离散型随机变量：Discrete RVs

![](/static/2021-10-21-05-11-10.png)
Discrete RVs defines by probabilities of di↵erent events taking place. E.g. probability of random variable X taking value x: **离散 RV 由不同事件发生的概率定义。例如。随机变量 X 取值 x 的概率**：

## 连续型随机变量：Continuous RVs

![](/static/2021-10-21-05-28-33.png)

* 通过density function密度函数可以计算概率
  * 不是概率，是区间内可能有多少个值？
  * 面积/积分，才是概率

# 联合概率和密度：Joint probabilities and densities

![](/static/2021-10-21-05-35-50.png)

* 联合概率
* 离散RV，X，Y的联合概率
  * P(X=x, Y=y)
* 连续RV，X，Y的联合概率
  * p(x0,x1) （相乘）同时观察到所有这些数据的概率，所有观测数据点的联合概率分布 【假设每个数据点独立生成】
    * 逗号表示“与”，同时发生的概率
* 似然函数通常是多个观测数据发生的概率的联合概率，即多个观测数据都发生的概率
  * 单个观测数据发生的可能性为P(θ)，如果各个观测之间是相互独立的，那么多个观测数据都发生的概率可表示为各个样本发生的概率的乘积
  * 如果事件A和事件B相互独立，那么事件A和B同时发生的概率是A发生的概率 * B发生的概率
* 似然函数通常用`L`表示
  * 似然（Likelihood）函数表示的是基于观察的数据，取不同的参数`θ`时，统计模型**以多大的可能性接近真实观察数据**
  * ![](/static/2021-11-02-07-13-58.png)

# 耦合/独立：Dependence/Independence

![](/static/2021-10-21-22-15-02.png)
![](/static/2021-10-21-22-17-08.png)

* 独立/非独立会影响联合概率
* 独立离散RV的联合概率就是乘积

# 条件概率：Conditioning

当两个离散RV相互依赖

![](/static/2021-10-21-22-33-29.png)

* 因为相互依赖，可以利用条件概率

## 连续RV-条件

![](/static/2021-10-21-22-38-51.png)
![](/static/2021-11-02-05-46-13.png)

* 注意后面提了，tn也是个随机变量
* 注意 `|`表示的到底是条件概率还是概率分布的参数
  * 这里P算的是连续RV的联合概率，，最后算的也就是似然性

# 高斯模型:Gaussian model

连续型随机变量常用模型

![](/static/2021-10-21-22-48-11.png)

![](/static/2021-10-21-22-58-06.png)

线性回归的一大假设（之前的文章也有提到）是：**误差服从均值为0的正态分布，且多个观测数据之间互不影响，相互独立**

# Generating data

高斯模型怎么关联error terms的？（怎么关联原模型

![](/static/2021-10-21-22-58-54.png)

![](/static/2021-10-21-23-08-28.png)

* 通过probability distribution for the random var episilon
* 线性回归的一大假设（之前的文章也有提到）是：**误差服从均值为0的正态分布，且多个观测数据之间互不影响，相互独立**

---

![](/static/2021-10-21-23-22-17.png)

* 根据数据集，先计算模型预测值

![](/static/2021-10-21-23-23-09.png)

* 选取mean，std计算误差，附加在原模型（预测值）上
  * 假设已知

![](/static/2021-10-21-23-25-15.png)
![](/static/2021-10-21-23-32-47.png)

* 调整std

---

![](/static/2021-10-22-05-23-08.png)
![](/static/2021-11-02-07-23-24.png)
![](/static/2021-11-02-07-55-46.png)
![](/static/2021-11-02-07-58-29.png)
![](/static/2021-11-02-08-02-37.png)

* 误差项用mean=0的正态分布表示出来，可以进一步得到单个观测数据的似然性（此时还是假设mean=0, 希望找到一个w参数，使似然性越大，误差越小，越贴近真实数据）
* 公式3推的时候，先假设**误差项服从mean=0的正态分布，然后进一步表示误差项=yi(真实值) - w.T*xi(预测值)**
  * 此时 `x=yi`，`μ=w.T*xi`
  * 因此有 单个样本的概率（似然性），p(yi|xi;w) = N(w.T*xi, σ^2)，**概率密度函数**服从`μ=w.T*xi`的正态分布
    * <font color="deeppink">也就是下面那个另一种写法怎么来的</font>
    * **注意不是似然函数。。似然函数是。。涉及N个样本的联合概率**
* constant就是模型预测值w.T*xn

![](/static/2021-10-22-07-23-30.png)

* 也就是假设误差项服从mean=0的正态分布，多个数据之间独立，互不影响。
  * 单个观测值tn的概率（似然性）L就是就是关于tn的函数，未知参数是 `w`和`σ`
* 此时N个观测数据的似然（联合概率），N个观测值的概率乘积，未知参数是 `w`和`σ`（能表示出w，σ也好求了，虽然我们只关心参数ω取何值时，似然函数最大，标准差σ并不会影响ω取何值时似然函数最大，所以可以忽略掉带有标准差σ的项）
  * 似然性p(t|xn,w,σ^2) = N个观测数据的联合概率 = N个观测值的概率乘积 = 密度函数表示就是：N(w.T\*xn, σ^2)，服从mean=w.T*xn的正态分布
  * 因为当μ=0时，有 N(μ,σ^2) + w.T*xn = N(μ+w.T*xn, σ^2)

# 随机变量t：t is a random variable too

![](/static/2021-10-22-08-15-18.png)

* t预测值本身也是个RV，可以有密度函数 （似然性越高，t预测越接近真实值）
  * 但是tn固定（数据集）

![](/static/2021-10-22-08-26-52.png)
![](/static/2021-10-27-02-32-49.png)

* 冲突：给定数据集，tn已知（固定），但是t又存在概率密度？ --- 其实这种情况下，求得是**似然性**
  * 似然性 - 未知参数的函数
* 似然性越高，模型越好？（预测值越接近真实值，越可能发生）
* 密度函数
  * 告诉给定一个xn，，生成特定tn的似然性？

# 似然性：Likelihood

![](/static/2021-10-22-08-55-03.png)

* 找到最优配置最大化tn的似然性

![](/static/2021-10-22-08-36-27.png)

model 1
![](/static/2021-10-22-08-45-57.png)
![](/static/2021-10-22-08-46-15.png)

model 2
![](/static/2021-10-22-08-46-59.png)
![](/static/2021-10-22-08-47-15.png)

model 3
![](/static/2021-10-22-08-48-16.png)

* higher likelihood is the better fit model would be
* 似然性越高，模型越好

# Summary

![](/static/2021-10-22-08-57-18.png)

# 补充：如何生成随机变量的高斯概率密度模型

![](/static/2021-10-22-09-20-37.png)
![](/static/2021-10-22-09-29-18.png)
![](/static/2021-10-22-09-29-27.png)

# ===============

# 似然性例子：Likelihood Example

两个数据点

![](/static/2021-10-27-02-37-44.png)

![](/static/2021-10-27-02-38-10.png)
![](/static/2021-10-27-02-38-18.png)

![](/static/2021-10-27-02-38-41.png)

* model3似然性最高
* 合并两个点（多个点）
  * 因为t1,t2都是连续型RV，可以表示其联合密度，相当于t1,t2密度的product【此时要假设多个数据之间相互独立】
  * 泛化这个步骤->合并所有t（tn）（得到N个样本的似然性，即联合概率）

# 似然性优化：Likelihood Optimisation

![](/static/2021-10-27-03-40-13.png)

* 假设tn之间是条件（conditionally）**独立的**，，假设来自同一模型（w, σ相同）
  * 就可以表示出联合概率

![](/static/2021-10-27-03-41-43.png)

* 找到最大化似然性的配置w

## 为什么用Log

![](/static/2021-10-27-03-48-11.png)
![](/static/2021-10-27-03-47-27.png)
![](/static/2021-11-02-06-09-50.png)
![](/static/2021-11-02-07-55-59.png)
![](/static/2021-11-02-07-58-15.png)
![](/static/2021-11-02-08-02-59.png)

* 为什么改写成对Log似然性的优化
* 单个的话就是，，单样本发生的概率，，，似然性
* 似然函数是N个样本的联合概率乘积

# 最佳参数：Optimum parameters

![](/static/2021-11-02-08-02-57.png)
![](/static/2021-10-27-03-49-59.png)
![](/static/2021-10-27-03-50-16.png)

![](/static/2021-10-27-03-54-54.png)

* 这个配置预测有多少可信度？

# ====================

# MAP

# 参数估计可信度：Confidence in parameter estimates

![](/static/2021-10-27-03-59-30.png)

* infinite data情况下 - true para
  * 但我们希望数据越多，estimates参数能够往true para收敛
* 如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样

![](/static/2021-10-27-04-03-07.png)

* are they correct
  * 有效性
* if we could keep adding data, would we converge on the true value?
  * 一致性
* **使用的数据集部分不同，参数预测不同（w参数预测可变性非常大，这种variability告诉参数的置信度有多少**），可信度也不同
  * 比如参数预测1->0,, 0->10，对于他们的可信度肯定不同
  * 样本集不同，分布不同，参数也不同。不知道到底属于哪个分布，也不知道到底什么分布参数是准确的
* **以上步骤，需要学习好几次（不同数据集）。。需要引入期望值来简化（直接计算estimate可信度?**
  * 评估estimate?

# 期望值：Expectations

## 采样：Sampling

计算可信度

![](/static/2021-10-27-04-51-33.png)

* 概率分布为p(x)的随机变量X，想求它的均值
* 可以**采样**计算
  * 利用概率分布生成样本，然后求样本均值
* 这么计算，样本量不够，，
  * 但是可以直接计算**期望值**

## 离散型RV期望值

![](/static/2021-10-27-05-00-16.png)

* 如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样
* 离散，连加

## 连续型RV期望值

![](/static/2021-10-27-05-01-26.png)

* 定积分
* 假设连续RV是均匀分布的

## general - 随机变量函数

x - scalar RV （一元）

![](/static/2021-10-27-05-10-56.png)
![](/static/2021-11-03-08-04-05.png)

* mean
* variance
  * 比如一个数据集上数据与其期望预测（均值）之间差异的平方的期望？

x - vector/matrix RV （多元）

![](/static/2021-10-27-05-40-44.png)

## Gaussians Distribution例子

![](/static/2021-10-27-05-42-50.png)

* 单个随机变量
  * 方差
* 多个随机变量
  * 协方差

# E{w} & cov{w}

![](/static/2021-11-03-18-48-02.png)

![](/static/2021-10-27-05-47-57.png)

* 注意此时这个`sigma^2`的`w`假设已给定，不是RV

![](/static/2021-10-27-07-02-35.png)

* 注意 `Ep(t|X, w, sigma^2){w_hat}`
  * `w`是true para，假设已知
* 列出w_hat的期望，可以评估w_hat与w（true para）之间的差异（偏差？）

![](/static/2021-10-27-07-11-15.png)

* w - (condition - true para)
* w_hat - estimate
* 连续RV期望=>积分: E(w_hat) = w(true para)
  * 数据越多，参数估计(w_hat) 越接近true para 【期望w_hat无偏差】
* 协方差，通常sigma^2带入估计

# cov{w}

![](/static/2021-11-03-19-07-46.png)
![](/static/2021-11-03-19-08-00.png)
![](/static/2021-10-27-07-19-26.png)

* a - w0的variance
* c - w1的variance
* b - (线性）相关性 correlation
* a,c - w0,w1还能改多少的情况下仍能拟合
* b
  * 如 <0, a!=c, w0, w1 negatively correlated. increase w0 may cause b unit loss in w1

![](/static/2021-10-27-07-28-27.png)

* 如果使用同一数据集，可以看出parameter estimate的估计（对比expectation=>(趋于)），，因为只有sigma^2会改变

## 例子

![](/static/2021-10-27-07-36-59.png)

* mean - w_hat(expectation)
* sigma^2 - covariance

# Summary

![](/static/2021-10-27-07-37-11.png)

* w_hat is unbiased -> feeding more data, get more good expectation performance
* 协方差，每个参数与参数估计期望的差异平方

# 预测：Predictions

![](/static/2021-10-27-08-43-35.png)

![](/static/2021-10-27-09-05-32.png)

* 用期望量化参数不确定性 - 参数估计
* estimate - w_hat
* tnew，应该是换个数据集（测试集）
  * tnew = ((X.T*X)^-1\*X.T\*t)*xnew
    * `t`是RV
  * 因此tnew是关于RV `t`的函数，tnew也是RV。可以表示其期望 & 方差（tnew是标量）

![](/static/2021-10-27-09-08-38.png)

* 期望数据越多，w_hat是w(true para)
* 进而，期望数据越多，模型预测平均（期望）接近真实值

## 方差：Prediction & Variance

![](/static/2021-10-27-09-12-18.png)

* tnew的方差与参数估计的协方差矩阵有关系
* 因此，如果参数方差很大，预测值方差也很大，【希望参数估计误差尽量的小

# 预测例子

![](/static/2021-10-27-09-16-54.png)

* error bar - tnew +- variance{tnew}
* 95%置信区间 - e{tnew} +- 2std{tnew}

![](/static/2021-10-27-09-17-08.png)

![](/static/2021-10-27-09-26-25.png)

## Not complex enough model – more ‘noise’

![](/static/2021-10-27-09-28-18.png)

* 低阶模型，方差很大取决于noise level
* sigma^2过大，最小损失很大

## Too complex model – parameters not well defined

![](/static/2021-10-27-09-30-50.png)

* 此时sigma^2很小，但是X size更大，过拟合

# Olympic prediction

![](/static/2021-10-27-09-32-48.png)

![](/static/2021-10-27-09-35-21.png)

# 可以基于似然性选择模型吗：Can we use likelihood to choose models?

![](/static/2021-10-27-09-37-35.png)

* 不能
  * 类似高阶模型的loss越来越小，但不是最好的
  * 越复杂模型，似然性越高，estimate noise level（sigma^2）越小 [但此时易过拟合，X size很大]

# Summary

![](/static/2021-10-27-09-41-53.png)