# Content

* [Content](#content)
* [Overview](#overview)
* [Linear Model in Vector Form](#linear-model-in-vector-form)
* [向量表示参数配置：Encapsulate parameters in a vector](#向量表示参数配置encapsulate-parameters-in-a-vector)
* [模型向量表示：Vector model](#模型向量表示vector-model)
  * [Loss & Mean Loss (Average Squared Loss)](#loss--mean-loss-average-squared-loss)
  * [What is q?](#what-is-q)
  * [Matrices](#matrices)
* [Summary:为什么用matrix & Vector表示](#summary为什么用matrix--vector表示)
* [===========](#)
* [Different models, same loss](#different-models-same-loss)
* [最小化损失：Minimising the loss](#最小化损失minimising-the-loss)
* [矩阵表示loss后求偏导总结](#矩阵表示loss后求偏导总结)
  * [例子](#例子)
* [应用更复杂模型：More general models](#应用更复杂模型more-general-models)
* [Common basis functions h ( x )](#common-basis-functions-h--x-)
* [Local effect of RBF](#local-effect-of-rbf)
* [summary](#summary)
* [===========](#-1)
* [目标值表示：Making Predictions](#目标值表示making-predictions)
* [多项式模型预测例子](#多项式模型预测例子)
* [如何进行模型order的选择？](#如何进行模型order的选择)
  * [复杂模型损失减少：Loss always decreases with model complexity](#复杂模型损失减少loss-always-decreases-with-model-complexity)
* [泛化 & 过拟合：Generalisation and over-fitting](#泛化--过拟合generalisation-and-over-fitting)
* [数据集划分：Where can we get more data?](#数据集划分where-can-we-get-more-data)
  * [例子](#例子-1)
  * [交叉验证：划分策略 - cross validation](#交叉验证划分策略---cross-validation)
  * [留一法交叉验证:leave-one-out cross-validation](#留一法交叉验证leave-one-out-cross-validation)
    * [例子](#例子-2)
* [怎么知道到底基于哪个交叉验证方法来选择order？](#怎么知道到底基于哪个交叉验证方法来选择order)
* [交叉验证计算成本问题：Computational issues](#交叉验证计算成本问题computational-issues)
* [summary](#summary-1)
* [其他特征？Should we care about what x means](#其他特征should-we-care-about-what-x-means)
* [===========](#-2)
* [补充](#补充)
* [数据集-向量表示：Encapsulate data in a vector](#数据集-向量表示encapsulate-data-in-a-vector)
* [Matrix multiplication](#matrix-multiplication)
* [L（矩阵/向量表示）的偏导计算 Partial diff wrt vector](#l矩阵向量表示的偏导计算-partial-diff-wrt-vector)

# Overview

![](/static/2021-10-09-03-56-44.png)

* 为什么需要general formulation - 描述更复杂的模型，不仅限于直线

# Linear Model in Vector Form

![](/static/2021-10-09-03-57-29.png)
![](/static/2021-10-09-03-58-22.png)

* 上述多项式模型，如果要解需要针对每个参数进行loss微分，然后偏导=0解出方程，推出关系

# 向量表示参数配置：Encapsulate parameters in a vector

![](/static/2021-10-09-04-04-28.png)

如何表示？

![](/static/2021-10-09-04-05-53.png)

# 模型向量表示：Vector model

inner product,,线性模型 t=w0+w1x

![](/static/2021-10-09-04-09-06.png)
![](/static/2021-10-09-04-09-47.png)

## Loss & Mean Loss (Average Squared Loss)

![](/static/2021-10-09-04-16-00.png)
![](/static/2021-10-09-04-16-22.png)
![](/static/2021-10-09-04-17-40.png)

* 如何进一步向量化损失函数？
* 用`q`表示整个loss sqaure

## What is q?

![](/static/2021-10-09-04-33-47.png)

## Matrices

![](/static/2021-10-09-04-35-22.png)
![](/static/2021-10-09-04-37-29.png)
![](/static/2021-10-09-04-39-05.png)

# Summary:为什么用matrix & Vector表示

![](/static/2021-10-09-04-42-14.png)

# ===========

# Different models, same loss

![](/static/2021-10-09-04-52-11.png)

* 损失函数可适用多种不同的模型（w, X配置不同）
* We can get an expression for the w that minimises L, that will work for any of these models. 我们可以得到一个最小化 L 的 w 表达式，它适用于这些模型中的任何一个。

# 最小化损失：Minimising the loss

![](/static/2021-10-09-04-55-19.png)

---

![](/static/2021-10-09-04-58-05.png)
![](/static/2021-10-09-05-05-09.png)

![](/static/2021-10-09-05-05-57.png)

* 注意，最后偏导shape - K * 1的列向量。所以中间项应该也为K * 1
  * K - 参数w的个数
* term1偏导的shape
  * ![](/static/2021-10-09-05-07-50.png)
* term2偏导的shape
  * ![](/static/2021-10-09-05-09-04.png)

---

最后，得出偏导，=0时，求出关系

![](/static/2021-10-09-05-10-48.png)
![](/static/2021-10-09-05-11-28.png)

因为xT * x （inner product，形状为K*K），

![](/static/2021-10-09-05-14-49.png)

* 进行inverse,推出w表示

---

![](/static/2021-10-09-05-01-34.png)

* W - (K * 1)
* X - （N * K），，K个w参数，，N是x1...xn

# 矩阵表示loss后求偏导总结

运用matrix转置，反矩阵，单位矩阵

![](/static/2021-10-09-05-17-57.png)
![](/static/2021-10-09-05-18-52.png)

* 求出来的w关系，又称为least square solution

## 例子

线性模型

![](/static/2021-10-09-05-24-56.png)

二次式模型

![](/static/2021-10-09-05-25-50.png)

8th order model

![](/static/2021-10-09-05-27-36.png)

# 应用更复杂模型：More general models

X general表示

![](/static/2021-10-09-05-40-11.png)
![](/static/2021-10-09-06-25-47.png)

* 不同的version的x的函数
* 每一行还是代表一个training data
  * 不同在于，每列的值取决于h（x） & x值

# Common basis functions h ( x )

![](/static/2021-10-09-06-28-39.png)

![](/static/2021-10-09-06-29-50.png)

# Local effect of RBF

![](/static/2021-10-09-06-44-01.png)
![](/static/2021-10-09-06-43-46.png)

# summary

![](/static/2021-10-09-06-45-21.png)

# ===========

# 目标值表示：Making Predictions

![](/static/2021-10-09-06-52-17.png)

* 最小二乘法解w - 用来min损失
* 模型结构取决于X矩阵（design matrix
  * 训练集中每个数据都应用不同h(x)

# 多项式模型预测例子

![](/static/2021-10-09-06-54-10.png)
![](/static/2021-10-09-06-54-32.png)

* 模型的选择很重要

# 如何进行模型order的选择？

类似w配置的选择，

![](/static/2021-10-09-06-56-46.png)
![](/static/2021-10-09-07-06-13.png)
![](/static/2021-10-09-07-06-58.png)

* 但是不一定能通过最小loss来选择order
* order越大（模型越复杂），loss总是在减少
  * = = 但是之前8th那个不太好
* **不知道目标值的情况下，能不能获取其他数据来预测**？
  * 数据集划分 - 训练集 & 测试集

## 复杂模型损失减少：Loss always decreases with model complexity

![](/static/2021-10-09-06-58-18.png)
![](/static/2021-10-09-06-59-12.png)
![](/static/2021-10-09-06-59-21.png)
![](/static/2021-10-09-06-59-42.png)

* 过拟合训练数据（noise），测试集可能完全不这样

# 泛化 & 过拟合：Generalisation and over-fitting

![](/static/2021-10-09-07-04-41.png)
![](/static/2021-10-09-07-05-28.png)

* 泛化(Generalization) 泛化是指模型很好地拟合**以前未见过的新数据【测试集**（从用于创建该模型的同一分布中抽取）的能力
* 过拟合模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。过拟合是由于模型的复杂程度超出所需程度而造成的。机器学习的基本冲突是适当拟合我们的数据，但也要尽可能简单地拟合数据
* Fitting a model perfectly to the training data is likely to lead to poor predictions because there will almost always be noise present.将模型完美地拟合到训练数据可能会导致预测不佳，因为几乎总是存在噪声。

# 数据集划分：Where can we get more data?

![](/static/2021-10-09-07-09-30.png)

## 例子

测试集sqaured loss表示

![](/static/2021-10-09-07-11-22.png)
![](/static/2021-10-09-07-13-37.png)

## 交叉验证：划分策略 - cross validation

![](/static/2021-10-09-07-16-06.png)
![](/static/2021-10-09-07-17-37.png)

* 交叉验证

## 留一法交叉验证:leave-one-out cross-validation

![](/static/2021-10-09-07-22-13.png)

* 交叉验证可重复进行
* 留一法
  * 留一法Leave-one-out Cross-validation：是一种特殊的交叉验证方式。顾名思义，如果样本容量为n，则k=n，进行n折交叉验证，每次留下一个样本进行验证。主要针对小样本数据

### 例子

![](/static/2021-10-09-07-25-27.png)

# 怎么知道到底基于哪个交叉验证方法来选择order？

![](/static/2021-10-09-07-30-44.png)

# 交叉验证计算成本问题：Computational issues

![](/static/2021-10-09-07-33-10.png)

# summary

* Showed how we can make predictions with our ‘linear’ model.展示了我们如何使用“线性”模型进行预测。
* Saw how choice of model has big influence in quality of predictions. 了解模型的选择如何对预测质量产生重大影响。
* Saw how the loss on the training data, L, cannot be used to choose models. 看到了训练数据 L 的损失如何不能用于选择模型。
  * Making model more complex always decreases the loss. 使模型更复杂总是会减少损失。
* Introduced the idea of using some data for validation. 引入了使用一些数据进行验证的想法。
* Introduced cross validation and leave-one-out cross validation. 引入了交叉验证和留一法交叉验证。

# 其他特征？Should we care about what x means

![](/static/2021-10-09-07-36-37.png)
![](/static/2021-10-09-07-36-56.png)
![](/static/2021-10-09-07-38-34.png)

* 之前x是year
* **尝试在design matrix中利用所有可能的特征？**

# ===========

# 补充

![](/static/2021-10-09-07-47-57.png)

# 数据集-向量表示：Encapsulate data in a vector

![](/static/2021-10-09-07-48-55.png)

# Matrix multiplication

![](/static/2021-10-09-07-52-47.png)
![](/static/2021-10-09-07-52-58.png)

* inner product

# L（矩阵/向量表示）的偏导计算 Partial diff wrt vector

![](/static/2021-10-09-07-53-47.png)

![](/static/2021-10-09-08-02-04.png)

应用inverse，推出w解

![](/static/2021-10-09-08-05-49.png)