# Lab 6-7

## Tutorial: 梯度下降法

background
![](/static/2020-11-16-03-18-57.png)

* 优化问题
  * 参数
  * 约束
  * 目标函数
* 优化算法
  * **超参数**
    * 它决定了如何搜索最佳参数设置(例如，当试图向下移动一个函数的梯度时，需要采取多大的步骤)
* 我们将使用**可微目标函数**，
  * 在这里我们可以计算函数在任何点的梯度，并使用这些信息快速移动到最小值

## Hill Climbing

![](/static/2020-11-16-03-34-05.png)

随机爬山搜索stochastic hill-climbing search

* 针对参数θ，进行轻微调整，计算loss

## Moon Phase Problem

f输入：图片，输出：月相，参数θ

![](/static/2020-11-16-04-27-54.png)

### Approximation

逼近法

当尝试逼近函数时，可以有以下形式的目标函数

![](/static/2020-11-16-04-28-33.png)

* 尝试找到参数θ，最小化 预期输出&实际输出的差异（||f(x;θ)-y||）
  * 输入`x`
* 我们将建立一个简单的“深度学习”系统。我们将完全忽略机器学习中的许多重要问题，比如过度拟合、规则化、高效的网络架构和公平评估，而是专注于使用一阶优化来找到一个近似于已知变换的函数。

![](/static/2020-11-16-05-18-36.png)

已知`y`是**预期输出**（每个图片的**月相**）

* 如`moon[15,:,:]`是30度月相

🍊 Remapping y

神经网会将预期影射到【-1，1】范围，预期输出y必须缩放至该范围

* 困难，
* **可以考虑映射至2D空间**
  * 以给定角度（每个角度对应了一个月相），固定长度的向量表示
  * `𝐲=[𝑟cos(𝑎),−𝑟sin(𝑎)]`
    * `r`-**随机缩放因子**
    * `a`-月相对应的**角度**

如`r=0.5`时，输出`181x2`的矩阵

* 行列式为`y`（预期月相至2D空间的映射）
  * 即，每行`y`为一个**月相**

## Deep Network

如何参数化函数？

* 参数化函数 -> 给定度数，输出月相图片？
  * 将输出`y`月相，映射到2D空间。给定一个角度`α`，可以得到特定月相输出`y`

🍊 以上参数化后的函数如何将images映射到月相？

* 使用 **simple deep nueral network predictor. 一个algorithms**
  * **接收一个向量输入，重复以下操作**
  * 添加小常量
  * `tanh()`**作为resulting vector; 压缩vector上所有元素，使他们至于`[-1,1]`范围**
  * **matrix** **乘** 该**结果向量**
* 需要定义用于变换向量的matrix形状。
  * 但是我们要优化找到这些矩阵中的元素。这是“学习”的部分。
  * 这些步骤中的每一步传统上被称为**预测函数的“层”**

![](/static/2020-11-16-05-36-56.png)

### Flattening & Unflattening

🍊 更好优化预测函数 --- 参数压缩（成向量），参数解压（成矩阵，shape特定）

![](/static/2020-11-16-05-40-02.png)

* 为了优化预测函数
  * 将所有**会变化**的things打包进 **单个flat参数向量`θ`**
* `predict()`
  * 如果给定正确`unflatten`参数，可以，从参数`θ`中解压出**Matrix列表（用于乘resulting vector的那个）**。`unflatten(theta)`
* `flatten`
  * 可以使用`flatten`可信函数使该步骤化简 --- 【自动梯度下降】
  * `theta, unflatten = flatten(list_of_matrices)`
  * 将**Matrix列表打包**进1D向量参数`theta`，`unflatten`参数只是用来解压（无关数据）

![](/static/2020-11-16-05-47-40.png)

## Network Architecture

![](/static/2020-11-16-05-56-56.png)

* 对于预测，矩阵shape的选择影响如何建模transformation
  * 如，我们有625维输入（25x25图片unraveled into flat vetors）& 2-d输出
* 然而，我们可以在预测函数中引入任意数量的中间矩阵
  * 这使得可以调整的参数更多，映射学习的灵活性更大; 这使得预测功能更加灵活

一个非常简单的模型可能只有一个625,2矩阵; 这将是一个简单的输入线性映射
![](/static/2020-11-16-06-02-16.png)

更复杂的模型
![](/static/2020-11-16-06-02-24.png)
这种“架构”将400维的输入向量映射到32维空间，然后映射到16维空间，然后映射到8维空间，最后映射到2维输出。每个矩阵都必须有一个输出维度与下列矩阵的输入维度相匹配。矩阵 w [0] ，w [1] ，w [2] ，w [3]指定了每一层的向量如何映射到下一层。**为了优化，所有这些矩阵 w [ i ]都被压平为一个向量 θ ——这只是对它们的元素进行了重新塑形。**

* `flatten(theta)`

我们不知道这些矩阵应该包含哪些值; 它们在每个步骤中都指定了一些未知的向量变换。我们必须优化这些矩阵的元素。
![](/static/2020-11-16-06-04-00.png)

这些矩阵形状的选择并不是非常重要(我刚刚做了上面的那些) ; 但是更多的矩阵元素意味着一个更灵活的函数，可以学习更复杂的东西; 但是更难有效地优化。

对于这个lab，假设要使用的矩阵形状是:
`625,32 32, 16 16, 8 8,2`
![](/static/2020-11-16-06-04-11.png)